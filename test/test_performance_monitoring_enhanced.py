"""
æµ‹è¯•PATEOASæ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡ç³»ç»Ÿ - ä»»åŠ¡6.3
"""

import time
import json
from datetime import datetime, timedelta
from aceflow.pateoas.enhanced_engine import PATEOASEnhancedEngine
from aceflow.pateoas.performance_monitor import PATEOASPerformanceMonitor, PerformanceMetric


def test_performance_monitor_initialization():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–"""
    print("ğŸ§ª æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–")
    
    monitor = PATEOASPerformanceMonitor("test_project")
    
    # éªŒè¯åˆå§‹åŒ–çŠ¶æ€
    assert monitor.project_id == "test_project"
    assert monitor.current_metrics['total_requests'] == 0
    assert monitor.current_metrics['successful_requests'] == 0
    assert len(monitor.component_performance) == 0
    
    print("âœ“ æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–æ­£å¸¸")
    return True


def test_operation_timing():
    """æµ‹è¯•æ“ä½œè®¡æ—¶åŠŸèƒ½"""
    print("\nâ±ï¸ æµ‹è¯•æ“ä½œè®¡æ—¶åŠŸèƒ½")
    
    monitor = PATEOASPerformanceMonitor("test_project")
    
    # æµ‹è¯•æ“ä½œè®¡æ—¶
    op_id = monitor.start_operation("test_operation")
    time.sleep(0.1)  # æ¨¡æ‹Ÿæ“ä½œè€—æ—¶
    monitor.end_operation(op_id, "test_component", success=True)
    
    # éªŒè¯ç»„ä»¶æ€§èƒ½è®°å½•
    assert "test_component" in monitor.component_performance
    comp_perf = monitor.component_performance["test_component"]
    assert comp_perf.total_calls == 1
    assert comp_perf.successful_calls == 1
    assert comp_perf.average_time > 0.05  # åº”è¯¥å¤§äº50ms
    
    print(f"  - ç»„ä»¶è°ƒç”¨æ¬¡æ•°: {comp_perf.total_calls}")
    print(f"  - æˆåŠŸè°ƒç”¨æ¬¡æ•°: {comp_perf.successful_calls}")
    print(f"  - å¹³å‡æ‰§è¡Œæ—¶é—´: {comp_perf.average_time:.4f}s")
    print(f"  - æˆåŠŸç‡: {comp_perf.success_rate:.2%}")
    
    print("âœ“ æ“ä½œè®¡æ—¶åŠŸèƒ½æ­£å¸¸")
    return True


def test_metrics_recording():
    """æµ‹è¯•æŒ‡æ ‡è®°å½•åŠŸèƒ½"""
    print("\nğŸ“Š æµ‹è¯•æŒ‡æ ‡è®°å½•åŠŸèƒ½")
    
    monitor = PATEOASPerformanceMonitor("test_project")
    
    # è®°å½•å„ç§æŒ‡æ ‡
    monitor.record_decision_accuracy(0.85)
    monitor.record_user_satisfaction(0.9)
    monitor.record_memory_efficiency(0.75)
    
    # éªŒè¯æŒ‡æ ‡è®°å½•
    assert monitor.current_metrics['decision_accuracy'] == 0.85
    assert monitor.current_metrics['user_satisfaction'] == 0.9
    assert monitor.current_metrics['memory_efficiency'] == 0.75
    
    # éªŒè¯æŒ‡æ ‡å†å²
    assert len(monitor.metrics_history) >= 3
    
    print(f"  - å†³ç­–å‡†ç¡®æ€§: {monitor.current_metrics['decision_accuracy']:.2%}")
    print(f"  - ç”¨æˆ·æ»¡æ„åº¦: {monitor.current_metrics['user_satisfaction']:.2%}")
    print(f"  - è®°å¿†æ•ˆç‡: {monitor.current_metrics['memory_efficiency']:.2%}")
    print(f"  - æŒ‡æ ‡å†å²è®°å½•æ•°: {len(monitor.metrics_history)}")
    
    print("âœ“ æŒ‡æ ‡è®°å½•åŠŸèƒ½æ­£å¸¸")
    return True


def test_performance_summary():
    """æµ‹è¯•æ€§èƒ½æ‘˜è¦ç”Ÿæˆ"""
    print("\nğŸ“‹ æµ‹è¯•æ€§èƒ½æ‘˜è¦ç”Ÿæˆ")
    
    monitor = PATEOASPerformanceMonitor("test_project")
    
    # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
    op_id = monitor.start_operation("test_op")
    time.sleep(0.05)
    monitor.end_operation(op_id, "test_component", True)
    
    monitor.record_decision_accuracy(0.88)
    monitor.record_user_satisfaction(0.92)
    
    # ç”Ÿæˆæ€§èƒ½æ‘˜è¦
    summary = monitor.get_performance_summary()
    
    # éªŒè¯æ‘˜è¦å†…å®¹
    assert 'current_metrics' in summary
    assert 'component_performance' in summary
    assert 'system_health' in summary
    assert 'alerts' in summary
    assert 'timestamp' in summary
    
    print(f"  - ç³»ç»Ÿå¥åº·åˆ†æ•°: {summary['system_health']:.2f}")
    print(f"  - ç»„ä»¶æ•°é‡: {len(summary['component_performance'])}")
    print(f"  - è­¦æŠ¥æ•°é‡: {len(summary['alerts'])}")
    
    print("âœ“ æ€§èƒ½æ‘˜è¦ç”Ÿæˆæ­£å¸¸")
    return True


def test_alert_system():
    """æµ‹è¯•è­¦æŠ¥ç³»ç»Ÿ"""
    print("\nğŸš¨ æµ‹è¯•è­¦æŠ¥ç³»ç»Ÿ")
    
    monitor = PATEOASPerformanceMonitor("test_project")
    
    # æ¨¡æ‹Ÿé«˜å“åº”æ—¶é—´è§¦å‘è­¦æŠ¥
    monitor.current_metrics['average_response_time'] = 3.0  # è¶…è¿‡è­¦å‘Šé˜ˆå€¼
    monitor.current_metrics['total_requests'] = 10
    monitor.current_metrics['successful_requests'] = 6  # 60%æˆåŠŸç‡ï¼Œä½äºè­¦å‘Šé˜ˆå€¼
    
    alerts = monitor._check_alerts()
    
    # éªŒè¯è­¦æŠ¥ç”Ÿæˆ
    assert len(alerts) > 0
    
    response_time_alerts = [a for a in alerts if a['metric'] == 'response_time']
    success_rate_alerts = [a for a in alerts if a['metric'] == 'success_rate']
    
    assert len(response_time_alerts) > 0
    assert len(success_rate_alerts) > 0
    
    print(f"  - æ€»è­¦æŠ¥æ•°: {len(alerts)}")
    for alert in alerts:
        print(f"  - {alert['type'].upper()}: {alert['message']}")
    
    print("âœ“ è­¦æŠ¥ç³»ç»Ÿæ­£å¸¸")
    return True


def test_performance_trends():
    """æµ‹è¯•æ€§èƒ½è¶‹åŠ¿åˆ†æ"""
    print("\nğŸ“ˆ æµ‹è¯•æ€§èƒ½è¶‹åŠ¿åˆ†æ")
    
    monitor = PATEOASPerformanceMonitor("test_project")
    
    # æ·»åŠ ä¸€ç³»åˆ—æŒ‡æ ‡æ¨¡æ‹Ÿè¶‹åŠ¿
    for i in range(20):
        value = 0.5 + (i * 0.02)  # é€’å¢è¶‹åŠ¿
        monitor._record_metric(f"test_metric", value, "performance", "ratio")
        time.sleep(0.001)  # å°å»¶è¿Ÿç¡®ä¿æ—¶é—´æˆ³ä¸åŒ
    
    # åˆ†æè¶‹åŠ¿
    trends = monitor._analyze_performance_trends()
    
    # éªŒè¯è¶‹åŠ¿åˆ†æ
    assert 'performance' in trends or 'status' in trends
    
    if 'performance' in trends:
        trend_info = trends['performance']
        print(f"  - æ€§èƒ½è¶‹åŠ¿: {trend_info['trend']}")
        print(f"  - å˜åŒ–ç™¾åˆ†æ¯”: {trend_info['change_percentage']:.1f}%")
        print(f"  - æ ·æœ¬æ•°é‡: {trend_info['sample_size']}")
    else:
        print(f"  - è¶‹åŠ¿åˆ†æçŠ¶æ€: {trends['status']}")
    
    print("âœ“ æ€§èƒ½è¶‹åŠ¿åˆ†ææ­£å¸¸")
    return True


def test_performance_report_generation():
    """æµ‹è¯•æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ"""
    print("\nğŸ“„ æµ‹è¯•æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ")
    
    monitor = PATEOASPerformanceMonitor("test_project")
    
    # æ·»åŠ æµ‹è¯•æ•°æ®
    op_id = monitor.start_operation("report_test")
    time.sleep(0.02)
    monitor.end_operation(op_id, "report_component", True)
    
    monitor.record_decision_accuracy(0.87)
    monitor.record_user_satisfaction(0.91)
    monitor.record_memory_efficiency(0.83)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = monitor.generate_performance_report()
    
    # éªŒè¯æŠ¥å‘Šç»“æ„
    required_sections = [
        'report_timestamp', 'project_id', 'summary', 
        'trends', 'recommendations', 'component_analysis'
    ]
    
    for section in required_sections:
        assert section in report, f"æŠ¥å‘Šç¼ºå°‘ {section} éƒ¨åˆ†"
    
    print(f"  - é¡¹ç›®ID: {report['project_id']}")
    print(f"  - æŠ¥å‘Šæ—¶é—´: {report['report_timestamp']}")
    print(f"  - å»ºè®®æ•°é‡: {len(report['recommendations'])}")
    print(f"  - ç»„ä»¶åˆ†ææ•°é‡: {len(report['component_analysis'])}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†å»ºè®®
    if report['recommendations']:
        print("  - æ€§èƒ½å»ºè®®:")
        for i, rec in enumerate(report['recommendations'][:3], 1):
            print(f"    {i}. {rec}")
    
    print("âœ“ æ€§èƒ½æŠ¥å‘Šç”Ÿæˆæ­£å¸¸")
    return True


def test_enhanced_engine_integration():
    """æµ‹è¯•å¢å¼ºå¼•æ“ä¸­çš„æ€§èƒ½ç›‘æ§é›†æˆ"""
    print("\nğŸ”— æµ‹è¯•å¢å¼ºå¼•æ“æ€§èƒ½ç›‘æ§é›†æˆ")
    
    engine = PATEOASEnhancedEngine("test_integration")
    
    # éªŒè¯æ€§èƒ½ç›‘æ§å™¨å·²åˆå§‹åŒ–
    assert hasattr(engine, 'performance_monitor')
    assert engine.performance_monitor.project_id == "test_integration"
    
    # æµ‹è¯•å¤„ç†è¯·æ±‚æ—¶çš„æ€§èƒ½ç›‘æ§
    result = engine.process_with_state_awareness(
        user_input="æµ‹è¯•æ€§èƒ½ç›‘æ§é›†æˆ",
        current_context={'test': True}
    )
    
    # éªŒè¯æ€§èƒ½æŒ‡æ ‡å·²æ›´æ–°
    assert engine.performance_metrics['total_requests'] > 0
    assert engine.performance_metrics['successful_requests'] > 0
    
    # è·å–æ€§èƒ½æ‘˜è¦
    summary = engine.get_performance_summary()
    assert 'current_metrics' in summary
    assert 'system_health' in summary
    
    print(f"  - æ€»è¯·æ±‚æ•°: {engine.performance_metrics['total_requests']}")
    print(f"  - æˆåŠŸè¯·æ±‚æ•°: {engine.performance_metrics['successful_requests']}")
    print(f"  - å¹³å‡å“åº”æ—¶é—´: {engine.performance_metrics['average_response_time']:.4f}s")
    print(f"  - ç³»ç»Ÿå¥åº·åˆ†æ•°: {summary['system_health']:.2f}")
    
    print("âœ“ å¢å¼ºå¼•æ“æ€§èƒ½ç›‘æ§é›†æˆæ­£å¸¸")
    return True


def test_user_satisfaction_calculation():
    """æµ‹è¯•ç”¨æˆ·æ»¡æ„åº¦è®¡ç®—"""
    print("\nğŸ˜Š æµ‹è¯•ç”¨æˆ·æ»¡æ„åº¦è®¡ç®—")
    
    engine = PATEOASEnhancedEngine("test_satisfaction")
    
    # æµ‹è¯•ä¸åŒè´¨é‡çš„ç»“æœ
    test_results = [
        {
            'confidence': 0.9,
            'alternative_actions': [{'action': 'alt1'}, {'action': 'alt2'}],
            'reasoning_chain': [{'step': 1}, {'step': 2}, {'step': 3}]
        },
        {
            'confidence': 0.5,
            'alternative_actions': [],
            'reasoning_chain': [{'step': 1}]
        }
    ]
    
    for i, result in enumerate(test_results, 1):
        satisfaction = engine._calculate_user_satisfaction(result)
        print(f"  - ç»“æœ{i}æ»¡æ„åº¦: {satisfaction:.2f}")
        assert 0.0 <= satisfaction <= 1.0
    
    print("âœ“ ç”¨æˆ·æ»¡æ„åº¦è®¡ç®—æ­£å¸¸")
    return True


def test_memory_efficiency_calculation():
    """æµ‹è¯•è®°å¿†æ•ˆç‡è®¡ç®—"""
    print("\nğŸ§  æµ‹è¯•è®°å¿†æ•ˆç‡è®¡ç®—")
    
    engine = PATEOASEnhancedEngine("test_memory_efficiency")
    
    # åˆ›å»ºæµ‹è¯•è®°å¿†
    from aceflow.pateoas.models import MemoryFragment, MemoryCategory
    
    memories = [
        MemoryFragment(
            content="é«˜é‡è¦æ€§è®°å¿†",
            category=MemoryCategory.LEARNING,
            importance=0.9
        ),
        MemoryFragment(
            content="ä¸­ç­‰é‡è¦æ€§è®°å¿†",
            category=MemoryCategory.DECISION,
            importance=0.6
        ),
        MemoryFragment(
            content="ä½é‡è¦æ€§è®°å¿†",
            category=MemoryCategory.PATTERN,
            importance=0.3
        )
    ]
    
    efficiency = engine._calculate_memory_efficiency(memories, "æµ‹è¯•æŸ¥è¯¢")
    
    print(f"  - è®°å¿†æ•°é‡: {len(memories)}")
    print(f"  - è®°å¿†æ•ˆç‡: {efficiency:.2f}")
    
    assert 0.0 <= efficiency <= 1.0
    
    print("âœ“ è®°å¿†æ•ˆç‡è®¡ç®—æ­£å¸¸")
    return True


def test_performance_persistence():
    """æµ‹è¯•æ€§èƒ½æ•°æ®æŒä¹…åŒ–"""
    print("\nğŸ’¾ æµ‹è¯•æ€§èƒ½æ•°æ®æŒä¹…åŒ–")
    
    monitor = PATEOASPerformanceMonitor("test_persistence")
    
    # æ·»åŠ æµ‹è¯•æ•°æ®
    monitor.record_decision_accuracy(0.85)
    monitor.record_user_satisfaction(0.9)
    
    # å¼ºåˆ¶ä¿å­˜
    monitor._save_metrics_history()
    
    # åˆ›å»ºæ–°çš„ç›‘æ§å™¨å®ä¾‹ï¼ŒéªŒè¯æ•°æ®åŠ è½½
    monitor2 = PATEOASPerformanceMonitor("test_persistence")
    
    # éªŒè¯æ•°æ®å·²åŠ è½½
    assert monitor2.current_metrics['decision_accuracy'] == 0.85
    assert monitor2.current_metrics['user_satisfaction'] == 0.9
    
    print(f"  - åŠ è½½çš„å†³ç­–å‡†ç¡®æ€§: {monitor2.current_metrics['decision_accuracy']:.2f}")
    print(f"  - åŠ è½½çš„ç”¨æˆ·æ»¡æ„åº¦: {monitor2.current_metrics['user_satisfaction']:.2f}")
    print(f"  - å†å²è®°å½•æ•°é‡: {len(monitor2.metrics_history)}")
    
    print("âœ“ æ€§èƒ½æ•°æ®æŒä¹…åŒ–æ­£å¸¸")
    return True


def test_component_performance_analysis():
    """æµ‹è¯•ç»„ä»¶æ€§èƒ½åˆ†æ"""
    print("\nğŸ”§ æµ‹è¯•ç»„ä»¶æ€§èƒ½åˆ†æ")
    
    monitor = PATEOASPerformanceMonitor("test_component_analysis")
    
    # æ¨¡æ‹Ÿä¸åŒç»„ä»¶çš„æ€§èƒ½æ•°æ®
    components = [
        ("state_manager", 0.1, True),
        ("memory_system", 0.2, True),
        ("flow_controller", 0.15, True),
        ("decision_gates", 0.3, False),  # ä¸€æ¬¡å¤±è´¥
        ("decision_gates", 0.25, True),
    ]
    
    for comp_name, exec_time, success in components:
        op_id = monitor.start_operation(f"test_{comp_name}")
        time.sleep(exec_time / 10)  # ç¼©çŸ­å®é™…ç­‰å¾…æ—¶é—´
        monitor.end_operation(op_id, comp_name, success)
    
    # åˆ†æç»„ä»¶æ€§èƒ½
    analysis = monitor._analyze_component_performance()
    
    print(f"  - åˆ†æçš„ç»„ä»¶æ•°é‡: {len(analysis)}")
    
    for comp_name, comp_analysis in analysis.items():
        print(f"  - {comp_name}:")
        print(f"    æ€§èƒ½åˆ†æ•°: {comp_analysis['performance_score']:.2f}")
        print(f"    å¯é æ€§: {comp_analysis['reliability']:.2%}")
        print(f"    çŠ¶æ€: {comp_analysis['status']}")
    
    print("âœ“ ç»„ä»¶æ€§èƒ½åˆ†ææ­£å¸¸")
    return True


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        test_performance_monitor_initialization,
        test_operation_timing,
        test_metrics_recording,
        test_performance_summary,
        test_alert_system,
        test_performance_trends,
        test_performance_report_generation,
        test_enhanced_engine_integration,
        test_user_satisfaction_calculation,
        test_memory_efficiency_calculation,
        test_performance_persistence,
        test_component_performance_analysis
    ]
    
    success_count = 0
    for test_func in tests:
        try:
            if test_func():
                success_count += 1
        except Exception as e:
            print(f"âŒ {test_func.__name__} å¤±è´¥: {e}")
    
    if success_count == len(tests):
        print(f"\nâœ… ä»»åŠ¡6.3 - æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡ æµ‹è¯•é€šè¿‡ ({success_count}/{len(tests)})")
        print("ğŸ¯ åŠŸèƒ½éªŒè¯:")
        print("  âœ“ æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å’Œé…ç½®")
        print("  âœ“ æ“ä½œè®¡æ—¶å’Œç»„ä»¶æ€§èƒ½è·Ÿè¸ª")
        print("  âœ“ å†³ç­–å‡†ç¡®æ€§å’Œç”¨æˆ·æ»¡æ„åº¦æŒ‡æ ‡")
        print("  âœ“ è®°å¿†æ•ˆç‡å’Œç³»ç»Ÿå¥åº·è¯„ä¼°")
        print("  âœ“ å®æ—¶è­¦æŠ¥å’Œé˜ˆå€¼ç›‘æ§")
        print("  âœ“ æ€§èƒ½è¶‹åŠ¿åˆ†æå’Œé¢„æµ‹")
        print("  âœ“ ç»¼åˆæ€§èƒ½æŠ¥å‘Šç”Ÿæˆ")
        print("  âœ“ å¢å¼ºå¼•æ“é›†æˆå’Œè‡ªåŠ¨ç›‘æ§")
        print("  âœ“ æ•°æ®æŒä¹…åŒ–å’Œå†å²è®°å½•")
        print("  âœ“ ç»„ä»¶çº§æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®")
    else:
        print(f"\nâŒ ä»»åŠ¡6.3 æµ‹è¯•å¤±è´¥ ({success_count}/{len(tests)} é€šè¿‡)")