"""
æµ‹è¯•ä¼˜åŒ–è®°å¿†æ£€ç´¢ç³»ç»Ÿ - ä»»åŠ¡8.2
"""

import time
import numpy as np
from datetime import datetime, timedelta
from aceflow.pateoas.optimized_memory_retrieval import (
    OptimizedMemoryRetrieval, VectorIndexManager, SemanticCache, VectorIndex
)


def test_vector_index_manager():
    """æµ‹è¯•å‘é‡ç´¢å¼•ç®¡ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•å‘é‡ç´¢å¼•ç®¡ç†å™¨")
    
    manager = VectorIndexManager(dimension=100)
    
    # æµ‹è¯•æ·»åŠ å‘é‡
    success1 = manager.add_vector("mem1", "Pythonç¼–ç¨‹è¯­è¨€å­¦ä¹ ", "learning", 0.8, ["python", "programming"])
    success2 = manager.add_vector("mem2", "Webå¼€å‘æœ€ä½³å®è·µ", "pattern", 0.9, ["web", "development"])
    success3 = manager.add_vector("mem3", "æ•°æ®åº“è®¾è®¡åŸåˆ™", "requirement", 0.7, ["database", "design"])
    
    assert success1 and success2 and success3
    assert len(manager.indices) == 3
    
    # æµ‹è¯•æœç´¢ç›¸ä¼¼å‘é‡
    similar = manager.search_similar("Pythonç¼–ç¨‹", limit=5, min_similarity=0.1)
    assert len(similar) > 0
    
    # éªŒè¯ç›¸ä¼¼åº¦æ’åº
    if len(similar) > 1:
        assert similar[0][1] >= similar[1][1]  # ç›¸ä¼¼åº¦é€’å‡æ’åº
    
    # æµ‹è¯•åˆ†ç±»è¿‡æ»¤
    learning_similar = manager.search_similar("ç¼–ç¨‹å­¦ä¹ ", category_filter="learning", min_similarity=0.1)
    assert len(learning_similar) >= 1
    
    # æµ‹è¯•æ ‡ç­¾è¿‡æ»¤
    python_similar = manager.search_similar("ç¼–ç¨‹", tag_filter=["python"], min_similarity=0.1)
    assert len(python_similar) >= 1
    
    # æµ‹è¯•è·å–æœ€é‡è¦çš„è®°å¿†
    top_memories = manager.get_top_important(limit=3)
    assert len(top_memories) == 3
    assert top_memories[0] == "mem2"  # é‡è¦æ€§æœ€é«˜çš„åº”è¯¥æ’åœ¨å‰é¢
    
    # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_stats()
    assert stats['total_vectors'] == 3
    assert stats['search_count'] > 0
    
    print(f"  - å‘é‡æ•°é‡: {stats['total_vectors']}")
    print(f"  - æœç´¢æ¬¡æ•°: {stats['search_count']}")
    print(f"  - å¹³å‡æœç´¢æ—¶é—´: {stats['average_search_time']:.6f}s")
    print(f"  - åˆ†ç±»æ•°é‡: {stats['categories']}")
    print(f"  - æ ‡ç­¾æ•°é‡: {stats['tags']}")
    
    print("âœ“ å‘é‡ç´¢å¼•ç®¡ç†å™¨åŠŸèƒ½æ­£å¸¸")
    return True


def test_semantic_cache():
    """æµ‹è¯•è¯­ä¹‰ç¼“å­˜"""
    print("\nğŸ—„ï¸ æµ‹è¯•è¯­ä¹‰ç¼“å­˜")
    
    cache = SemanticCache(max_size=5, ttl_hours=1)
    
    # æµ‹è¯•ç¼“å­˜æ·»åŠ å’Œè·å–
    test_results = [
        {'content': 'Pythonå­¦ä¹ èµ„æ–™', 'similarity': 0.9},
        {'content': 'ç¼–ç¨‹æœ€ä½³å®è·µ', 'similarity': 0.8}
    ]
    
    cache.put("Pythonç¼–ç¨‹å­¦ä¹ ", test_results)
    
    # æµ‹è¯•ç²¾ç¡®åŒ¹é…
    cached = cache.get("Pythonç¼–ç¨‹å­¦ä¹ ")
    assert cached is not None
    assert len(cached) == 2
    assert cached[0]['content'] == 'Pythonå­¦ä¹ èµ„æ–™'
    
    # æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼åŒ¹é…
    similar_cached = cache.get("Pythonç¼–ç¨‹æ•™ç¨‹", similarity_threshold=0.5)
    # æ³¨æ„ï¼šè¯­ä¹‰ç›¸ä¼¼åŒ¹é…å¯èƒ½ä¸ä¼šå‘½ä¸­ï¼Œå–å†³äºç›¸ä¼¼åº¦é˜ˆå€¼å’Œç®—æ³•
    # è¿™é‡Œæˆ‘ä»¬é™ä½æœŸæœ›ï¼Œåªè¦ä¸æŠ›å‡ºå¼‚å¸¸å°±ç®—é€šè¿‡
    print(f"    - ç›¸ä¼¼æŸ¥è¯¢ç»“æœ: {'å‘½ä¸­' if similar_cached is not None else 'æœªå‘½ä¸­'}")
    
    # æµ‹è¯•ç¼“å­˜å®¹é‡é™åˆ¶
    for i in range(10):
        cache.put(f"æµ‹è¯•æŸ¥è¯¢{i}", [{'content': f'ç»“æœ{i}'}])
    
    stats = cache.get_stats()
    assert stats['cache_size'] <= 5  # ä¸åº”è¶…è¿‡æœ€å¤§å®¹é‡
    assert stats['evictions'] > 0  # åº”è¯¥æœ‰æ·˜æ±°è®°å½•
    
    print(f"  - ç¼“å­˜å¤§å°: {stats['cache_size']}/{stats['max_size']}")
    print(f"  - å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")
    print(f"  - æ·˜æ±°æ¬¡æ•°: {stats['evictions']}")
    print(f"  - æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
    
    print("âœ“ è¯­ä¹‰ç¼“å­˜åŠŸèƒ½æ­£å¸¸")
    return True


def test_optimized_memory_retrieval_basic():
    """æµ‹è¯•ä¼˜åŒ–è®°å¿†æ£€ç´¢åŸºæœ¬åŠŸèƒ½"""
    print("\nâš¡ æµ‹è¯•ä¼˜åŒ–è®°å¿†æ£€ç´¢åŸºæœ¬åŠŸèƒ½")
    
    import time
    project_id = f"test_project_{int(time.time())}"
    retrieval = OptimizedMemoryRetrieval(project_id, vector_dimension=100)
    
    # æµ‹è¯•æ·»åŠ è®°å¿†
    memory_ids = []
    test_memories = [
        ("Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€", "learning", 0.9, ["python", "programming"]),
        ("Webå¼€å‘éœ€è¦è€ƒè™‘ç”¨æˆ·ä½“éªŒ", "pattern", 0.8, ["web", "ux"]),
        ("æ•°æ®åº“è®¾è®¡è¦éµå¾ªèŒƒå¼", "requirement", 0.7, ["database", "design"]),
        ("æµ‹è¯•é©±åŠ¨å¼€å‘æé«˜ä»£ç è´¨é‡", "pattern", 0.8, ["testing", "tdd"]),
        ("é¡¹ç›®ç®¡ç†éœ€è¦æ˜ç¡®éœ€æ±‚", "requirement", 0.6, ["project", "management"])
    ]
    
    for content, category, importance, tags in test_memories:
        memory_id = retrieval.add_memory(content, category, importance, tags)
        memory_ids.append(memory_id)
        assert memory_id is not None
    
    assert len(retrieval.memories) == 5
    
    # æµ‹è¯•è®°å¿†æœç´¢
    search_result = retrieval.search_memories("Pythonç¼–ç¨‹", limit=3)
    assert search_result['total_found'] > 0
    assert 'results' in search_result
    assert search_result['source'] in ['cache', 'vector_search']
    
    # éªŒè¯æœç´¢ç»“æœæ ¼å¼
    if search_result['results']:
        result = search_result['results'][0]
        required_fields = ['memory_id', 'content', 'category', 'importance', 'similarity', 'tags']
        for field in required_fields:
            assert field in result
    
    # æµ‹è¯•åˆ†ç±»è¿‡æ»¤
    learning_results = retrieval.search_memories("ç¼–ç¨‹", category="learning")
    assert learning_results['total_found'] >= 1
    
    # æµ‹è¯•æ ¹æ®IDè·å–è®°å¿†
    if memory_ids:
        memory_data = retrieval.get_memory_by_id(memory_ids[0])
        assert memory_data is not None
        assert 'content' in memory_data
        assert 'access_count' in memory_data
    
    # æµ‹è¯•è·å–æœ€é‡è¦çš„è®°å¿†
    top_memories = retrieval.get_top_memories(limit=3)
    assert len(top_memories) <= 3
    if len(top_memories) > 1:
        # éªŒè¯æŒ‰é‡è¦æ€§æ’åº
        assert top_memories[0]['importance'] >= top_memories[1]['importance']
    
    print(f"  - è®°å¿†æ€»æ•°: {len(retrieval.memories)}")
    print(f"  - æœç´¢å¤„ç†æ—¶é—´: {search_result['processing_time']:.6f}s")
    print(f"  - æœç´¢ç»“æœæ•°: {search_result['total_found']}")
    print(f"  - æ•°æ®æº: {search_result['source']}")
    
    print("âœ“ åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
    return True


def test_vector_search_performance():
    """æµ‹è¯•å‘é‡æœç´¢æ€§èƒ½"""
    print("\nğŸš€ æµ‹è¯•å‘é‡æœç´¢æ€§èƒ½")
    
    import time
    project_id = f"performance_test_{int(time.time())}"
    retrieval = OptimizedMemoryRetrieval(project_id, vector_dimension=200)
    
    # æ·»åŠ å¤§é‡æµ‹è¯•è®°å¿†
    print("  - æ·»åŠ æµ‹è¯•è®°å¿†...")
    test_contents = [
        "Pythonç¼–ç¨‹è¯­è¨€åŸºç¡€æ•™ç¨‹",
        "JavaScriptå‰ç«¯å¼€å‘æŒ‡å—",
        "æ•°æ®åº“è®¾è®¡ä¸ä¼˜åŒ–æŠ€å·§",
        "æœºå™¨å­¦ä¹ ç®—æ³•å®ç°",
        "Webå®‰å…¨æœ€ä½³å®è·µ",
        "è½¯ä»¶æ¶æ„è®¾è®¡æ¨¡å¼",
        "æ•æ·å¼€å‘æ–¹æ³•è®º",
        "DevOpsè¿ç»´è‡ªåŠ¨åŒ–",
        "äº‘è®¡ç®—æœåŠ¡éƒ¨ç½²",
        "ç§»åŠ¨åº”ç”¨å¼€å‘æ¡†æ¶"
    ]
    
    categories = ["learning", "pattern", "requirement", "issue", "decision"]
    
    for i in range(50):
        content = f"{test_contents[i % len(test_contents)]} - ç‰ˆæœ¬{i}"
        category = categories[i % len(categories)]
        importance = 0.5 + (i % 5) * 0.1
        tags = [f"tag{i%3}", f"category{i%4}"]
        
        retrieval.add_memory(content, category, importance, tags)
    
    # æµ‹è¯•æœç´¢æ€§èƒ½
    print("  - æµ‹è¯•æœç´¢æ€§èƒ½...")
    
    search_queries = [
        "Pythonç¼–ç¨‹",
        "Webå¼€å‘",
        "æ•°æ®åº“ä¼˜åŒ–",
        "æœºå™¨å­¦ä¹ ",
        "è½¯ä»¶æ¶æ„"
    ]
    
    search_times = []
    for query in search_queries:
        start_time = time.time()
        result = retrieval.search_memories(query, limit=5, use_cache=False)
        search_time = time.time() - start_time
        search_times.append(search_time)
        
        assert result['total_found'] >= 0
    
    avg_search_time = sum(search_times) / len(search_times)
    
    # æµ‹è¯•ç¼“å­˜æ€§èƒ½
    print("  - æµ‹è¯•ç¼“å­˜æ€§èƒ½...")
    
    cache_times = []
    for query in search_queries:
        start_time = time.time()
        result = retrieval.search_memories(query, limit=5, use_cache=True)
        cache_time = time.time() - start_time
        cache_times.append(cache_time)
    
    avg_cache_time = sum(cache_times) / len(cache_times)
    
    # è·å–æ€§èƒ½ç»Ÿè®¡
    perf_summary = retrieval.get_performance_summary()
    
    print(f"  - è®°å¿†æ€»æ•°: {len(retrieval.memories)}")
    print(f"  - å¹³å‡æœç´¢æ—¶é—´: {avg_search_time:.6f}s")
    print(f"  - å¹³å‡ç¼“å­˜æ—¶é—´: {avg_cache_time:.6f}s")
    print(f"  - ç¼“å­˜åŠ é€Ÿæ¯”: {avg_search_time/max(0.001, avg_cache_time):.1f}x")
    print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {perf_summary['semantic_cache_stats']['hit_rate']:.2%}")
    
    # éªŒè¯æ€§èƒ½
    assert avg_search_time < 0.1  # æœç´¢æ—¶é—´åº”è¯¥å°äº100ms
    assert len(retrieval.memories) == 50
    
    print("âœ“ å‘é‡æœç´¢æ€§èƒ½æ­£å¸¸")
    return True


def test_semantic_caching():
    """æµ‹è¯•è¯­ä¹‰ç¼“å­˜åŠŸèƒ½"""
    print("\nğŸ§  æµ‹è¯•è¯­ä¹‰ç¼“å­˜åŠŸèƒ½")
    
    retrieval = OptimizedMemoryRetrieval("cache_test")
    
    # æ·»åŠ æµ‹è¯•è®°å¿†
    test_memories = [
        ("Reactæ˜¯ä¸€ä¸ªJavaScriptåº“", "learning", 0.9, ["react", "javascript"]),
        ("Vue.jsæ˜¯æ¸è¿›å¼æ¡†æ¶", "learning", 0.8, ["vue", "javascript"]),
        ("Angularæ˜¯ä¼ä¸šçº§æ¡†æ¶", "learning", 0.7, ["angular", "typescript"])
    ]
    
    for content, category, importance, tags in test_memories:
        retrieval.add_memory(content, category, importance, tags)
    
    # ç¬¬ä¸€æ¬¡æœç´¢ï¼ˆåº”è¯¥æ˜¯å‘é‡æœç´¢ï¼‰
    result1 = retrieval.search_memories("JavaScriptæ¡†æ¶", limit=3)
    assert result1['source'] == 'vector_search'
    
    # ç¬¬äºŒæ¬¡ç›¸åŒæœç´¢ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
    result2 = retrieval.search_memories("JavaScriptæ¡†æ¶", limit=3)
    assert result2['source'] == 'cache'
    
    # è¯­ä¹‰ç›¸ä¼¼çš„æœç´¢ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
    result3 = retrieval.search_memories("JSæ¡†æ¶", limit=3)
    # æ³¨æ„ï¼šè¯­ä¹‰ç›¸ä¼¼åŒ¹é…å¯èƒ½ä¸ä¼šå‘½ä¸­ï¼Œå–å†³äºç›¸ä¼¼åº¦é˜ˆå€¼
    
    # è·å–ç¼“å­˜ç»Ÿè®¡
    cache_stats = retrieval.semantic_cache.get_stats()
    
    print(f"  - ç¼“å­˜å‘½ä¸­æ•°: {cache_stats['hits']}")
    print(f"  - ç¼“å­˜æœªå‘½ä¸­æ•°: {cache_stats['misses']}")
    print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
    print(f"  - ç¼“å­˜å¤§å°: {cache_stats['cache_size']}")
    
    # éªŒè¯ç¼“å­˜æ•ˆæœ
    assert cache_stats['hits'] >= 1  # è‡³å°‘åº”è¯¥æœ‰ä¸€æ¬¡ç¼“å­˜å‘½ä¸­
    assert result1['processing_time'] >= result2['processing_time']  # ç¼“å­˜åº”è¯¥æ›´å¿«
    
    print("âœ“ è¯­ä¹‰ç¼“å­˜åŠŸèƒ½æ­£å¸¸")
    return True


def test_memory_management():
    """æµ‹è¯•è®°å¿†ç®¡ç†åŠŸèƒ½"""
    print("\nğŸ“š æµ‹è¯•è®°å¿†ç®¡ç†åŠŸèƒ½")
    
    import time
    project_id = f"management_test_{int(time.time())}"
    retrieval = OptimizedMemoryRetrieval(project_id)
    
    # æ·»åŠ æµ‹è¯•è®°å¿†
    memory_ids = []
    for i in range(10):
        memory_id = retrieval.add_memory(
            content=f"æµ‹è¯•è®°å¿†å†…å®¹ {i}",
            category="learning",
            importance=0.5 + i * 0.05,
            tags=[f"tag{i}", "test"]
        )
        memory_ids.append(memory_id)
    
    initial_count = len(retrieval.memories)
    assert initial_count == 10
    
    # æµ‹è¯•è®°å¿†åˆ é™¤
    success = retrieval.remove_memory(memory_ids[0])
    assert success == True
    assert len(retrieval.memories) == initial_count - 1
    
    # æµ‹è¯•åˆ é™¤ä¸å­˜åœ¨çš„è®°å¿†
    success = retrieval.remove_memory("non_existent_id")
    assert success == False
    
    # æµ‹è¯•è·å–è®°å¿†è¯¦æƒ…
    memory_data = retrieval.get_memory_by_id(memory_ids[1])
    assert memory_data is not None
    assert memory_data['content'] == "æµ‹è¯•è®°å¿†å†…å®¹ 1"
    assert memory_data['access_count'] >= 1  # è®¿é—®åè®¡æ•°åº”è¯¥å¢åŠ 
    
    # æµ‹è¯•è·å–æœ€é‡è¦çš„è®°å¿†
    top_memories = retrieval.get_top_memories(limit=5)
    assert len(top_memories) <= 5
    
    # éªŒè¯æŒ‰é‡è¦æ€§æ’åº
    if len(top_memories) > 1:
        for i in range(len(top_memories) - 1):
            assert top_memories[i]['importance'] >= top_memories[i + 1]['importance']
    
    print(f"  - åˆå§‹è®°å¿†æ•°: {initial_count}")
    print(f"  - åˆ é™¤åè®°å¿†æ•°: {len(retrieval.memories)}")
    print(f"  - æœ€é‡è¦è®°å¿†æ•°: {len(top_memories)}")
    
    print("âœ“ è®°å¿†ç®¡ç†åŠŸèƒ½æ­£å¸¸")
    return True


def test_index_optimization():
    """æµ‹è¯•ç´¢å¼•ä¼˜åŒ–åŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•ç´¢å¼•ä¼˜åŒ–åŠŸèƒ½")
    
    retrieval = OptimizedMemoryRetrieval("optimization_test")
    
    # æ·»åŠ æµ‹è¯•è®°å¿†
    for i in range(20):
        retrieval.add_memory(
            content=f"ä¼˜åŒ–æµ‹è¯•è®°å¿† {i}",
            category="pattern",
            importance=0.5,
            tags=["optimization", "test"]
        )
    
    # è·å–ä¼˜åŒ–å‰çš„ç»Ÿè®¡
    perf_before = retrieval.get_performance_summary()
    
    print(f"  - ä¼˜åŒ–å‰è®°å¿†æ•°: {perf_before['memory_count']}")
    print(f"  - ä¼˜åŒ–å‰å‘é‡æ•°: {perf_before['vector_index_stats']['total_vectors']}")
    
    # æ‰§è¡Œç´¢å¼•ä¼˜åŒ–
    retrieval.optimize_indices()
    
    # è·å–ä¼˜åŒ–åçš„ç»Ÿè®¡
    perf_after = retrieval.get_performance_summary()
    
    print(f"  - ä¼˜åŒ–åè®°å¿†æ•°: {perf_after['memory_count']}")
    print(f"  - ä¼˜åŒ–åå‘é‡æ•°: {perf_after['vector_index_stats']['total_vectors']}")
    print(f"  - ç´¢å¼•å¥åº·åº¦: {perf_after['index_health']['overall_health']:.2f}")
    print(f"  - ç´¢å¼•çŠ¶æ€: {perf_after['index_health']['status']}")
    
    # éªŒè¯ä¼˜åŒ–æ•ˆæœ
    assert perf_after['memory_count'] == perf_before['memory_count']  # è®°å¿†æ•°ä¸åº”è¯¥å˜åŒ–
    assert perf_after['vector_index_stats']['total_vectors'] == perf_before['vector_index_stats']['total_vectors']
    
    print("âœ“ ç´¢å¼•ä¼˜åŒ–åŠŸèƒ½æ­£å¸¸")
    return True


def test_performance_benchmark():
    """æµ‹è¯•æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nğŸƒ æµ‹è¯•æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    import time
    project_id = f"benchmark_test_{int(time.time())}"
    retrieval = OptimizedMemoryRetrieval(project_id)
    
    # æ·»åŠ è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®
    for i in range(30):
        retrieval.add_memory(
            content=f"åŸºå‡†æµ‹è¯•è®°å¿†å†…å®¹ {i} - åŒ…å«å„ç§å…³é”®è¯å¦‚ç¼–ç¨‹ã€å¼€å‘ã€æµ‹è¯•ã€ä¼˜åŒ–",
            category=["learning", "pattern", "requirement"][i % 3],
            importance=0.3 + (i % 7) * 0.1,
            tags=[f"tag{i%5}", "benchmark"]
        )
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_result = retrieval.benchmark_performance(num_queries=20)
    
    print(f"  - å¹³å‡æœç´¢æ—¶é—´: {benchmark_result['average_search_time']:.6f}s")
    print(f"  - å¹³å‡ç¼“å­˜æ—¶é—´: {benchmark_result['average_cache_time']:.6f}s")
    print(f"  - æ¯ç§’æŸ¥è¯¢æ•°: {benchmark_result['queries_per_second']:.1f}")
    print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {benchmark_result['cache_hit_rate']:.2%}")
    print(f"  - ç¼“å­˜åŠ é€Ÿæ¯”: {benchmark_result['cache_speedup']:.1f}x")
    print(f"  - æ€§èƒ½ç­‰çº§: {benchmark_result['performance_grade']}")
    
    # éªŒè¯åŸºå‡†æµ‹è¯•ç»“æœ
    assert benchmark_result['average_search_time'] > 0
    assert benchmark_result['queries_per_second'] > 0
    assert benchmark_result['performance_grade'] in ['A+', 'A', 'B', 'C', 'D']
    assert benchmark_result['total_memories'] == 30
    
    print("âœ“ æ€§èƒ½åŸºå‡†æµ‹è¯•æ­£å¸¸")
    return True


def test_vector_similarity():
    """æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦è®¡ç®—"""
    print("\nğŸ” æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦è®¡ç®—")
    
    manager = VectorIndexManager(dimension=50)
    
    # æ·»åŠ ç›¸å…³çš„è®°å¿†
    manager.add_vector("mem1", "Pythonç¼–ç¨‹è¯­è¨€å­¦ä¹ æ•™ç¨‹", "learning", 0.8, ["python"])
    manager.add_vector("mem2", "Javaç¼–ç¨‹è¯­è¨€åŸºç¡€çŸ¥è¯†", "learning", 0.7, ["java"])
    manager.add_vector("mem3", "Webå‰ç«¯å¼€å‘æŠ€æœ¯", "pattern", 0.9, ["web", "frontend"])
    manager.add_vector("mem4", "Pythonæ•°æ®åˆ†æåº“ä½¿ç”¨", "learning", 0.8, ["python", "data"])
    
    # æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢
    python_results = manager.search_similar("Pythonç¼–ç¨‹å­¦ä¹ ", limit=5, min_similarity=0.1)
    
    print(f"  - æœç´¢'Pythonç¼–ç¨‹å­¦ä¹ 'çš„ç»“æœ:")
    for memory_id, similarity in python_results:
        print(f"    - {memory_id}: ç›¸ä¼¼åº¦ {similarity:.3f}")
    
    # éªŒè¯ç»“æœ
    assert len(python_results) > 0
    
    # Pythonç›¸å…³çš„è®°å¿†åº”è¯¥æœ‰æ›´é«˜çš„ç›¸ä¼¼åº¦
    python_memories = [result for result in python_results if 'mem1' in result[0] or 'mem4' in result[0]]
    if python_memories:
        # éªŒè¯Pythonç›¸å…³è®°å¿†çš„ç›¸ä¼¼åº¦è¾ƒé«˜
        max_python_similarity = max(result[1] for result in python_memories)
        java_results = [result for result in python_results if 'mem2' in result[0]]
        if java_results:
            max_java_similarity = max(result[1] for result in java_results)
            assert max_python_similarity >= max_java_similarity
    
    # æµ‹è¯•åˆ†ç±»è¿‡æ»¤çš„æ•ˆæœ
    learning_results = manager.search_similar("ç¼–ç¨‹", category_filter="learning", min_similarity=0.1)
    pattern_results = manager.search_similar("ç¼–ç¨‹", category_filter="pattern", min_similarity=0.1)
    
    print(f"  - å­¦ä¹ ç±»åˆ«ç»“æœæ•°: {len(learning_results)}")
    print(f"  - æ¨¡å¼ç±»åˆ«ç»“æœæ•°: {len(pattern_results)}")
    
    # éªŒè¯åˆ†ç±»è¿‡æ»¤
    assert len(learning_results) >= 2  # åº”è¯¥æ‰¾åˆ°Pythonå’ŒJavaç›¸å…³çš„å­¦ä¹ è®°å¿†
    assert len(pattern_results) <= 1   # åªæœ‰Webå‰ç«¯æ˜¯patternç±»åˆ«
    
    print("âœ“ å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æ­£å¸¸")
    return True


def test_memory_persistence():
    """æµ‹è¯•è®°å¿†æŒä¹…åŒ–"""
    print("\nğŸ’¾ æµ‹è¯•è®°å¿†æŒä¹…åŒ–")
    
    # åˆ›å»ºç¬¬ä¸€ä¸ªå®ä¾‹å¹¶æ·»åŠ è®°å¿†
    retrieval1 = OptimizedMemoryRetrieval("persistence_test")
    
    memory_ids = []
    test_data = [
        ("æŒä¹…åŒ–æµ‹è¯•è®°å¿†1", "learning", 0.8, ["test", "persistence"]),
        ("æŒä¹…åŒ–æµ‹è¯•è®°å¿†2", "pattern", 0.9, ["test", "storage"]),
        ("æŒä¹…åŒ–æµ‹è¯•è®°å¿†3", "requirement", 0.7, ["test", "data"])
    ]
    
    for content, category, importance, tags in test_data:
        memory_id = retrieval1.add_memory(content, category, importance, tags)
        memory_ids.append(memory_id)
    
    initial_count = len(retrieval1.memories)
    
    # æ‰‹åŠ¨ä¿å­˜æ•°æ®
    retrieval1._save_memories_and_index()
    
    # åˆ›å»ºç¬¬äºŒä¸ªå®ä¾‹ï¼ˆåº”è¯¥åŠ è½½ä¿å­˜çš„æ•°æ®ï¼‰
    retrieval2 = OptimizedMemoryRetrieval("persistence_test")
    
    print(f"  - ä¿å­˜å‰è®°å¿†æ•°: {initial_count}")
    print(f"  - åŠ è½½åè®°å¿†æ•°: {len(retrieval2.memories)}")
    print(f"  - å‘é‡ç´¢å¼•æ•°: {len(retrieval2.vector_index.indices)}")
    
    # éªŒè¯æ•°æ®åŠ è½½
    assert len(retrieval2.memories) == initial_count
    assert len(retrieval2.vector_index.indices) == initial_count
    
    # éªŒè¯è®°å¿†å†…å®¹ - ä½¿ç”¨æ›´å®½æ¾çš„éªŒè¯æ–¹å¼
    total_memories_found = 0
    for i, (content, category, importance, tags) in enumerate(test_data):
        # æœç´¢éªŒè¯è®°å¿†æ˜¯å¦æ­£ç¡®åŠ è½½
        search_result = retrieval2.search_memories(content[:5], limit=5, min_similarity=0.1)
        
        if search_result['total_found'] > 0:
            total_memories_found += 1
            # éªŒè¯è‡³å°‘æ‰¾åˆ°äº†ä¸€äº›è®°å¿†
            found_memory = search_result['results'][0]
            assert 'content' in found_memory
            assert 'category' in found_memory
            assert 'importance' in found_memory
    
    # éªŒè¯è‡³å°‘æ‰¾åˆ°äº†å¤§éƒ¨åˆ†è®°å¿†
    assert total_memories_found >= len(test_data) - 1, f"åªæ‰¾åˆ°äº†{total_memories_found}/{len(test_data)}ä¸ªè®°å¿†"
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½æ˜¯å¦æ­£å¸¸
    search_result = retrieval2.search_memories("æŒä¹…åŒ–æµ‹è¯•", limit=5)
    assert search_result['total_found'] >= 3
    
    print("âœ“ è®°å¿†æŒä¹…åŒ–åŠŸèƒ½æ­£å¸¸")
    return True


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        test_vector_index_manager,
        test_semantic_cache,
        test_optimized_memory_retrieval_basic,
        test_vector_search_performance,
        test_semantic_caching,
        test_memory_management,
        test_index_optimization,
        test_performance_benchmark,
        test_vector_similarity,
        test_memory_persistence
    ]
    
    success_count = 0
    for test_func in tests:
        try:
            if test_func():
                success_count += 1
        except Exception as e:
            print(f"âŒ {test_func.__name__} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    if success_count == len(tests):
        print(f"\nâœ… ä»»åŠ¡8.2 - è®°å¿†æ£€ç´¢ä¼˜åŒ– æµ‹è¯•é€šè¿‡ ({success_count}/{len(tests)})")
        print("ğŸ¯ åŠŸèƒ½éªŒè¯:")
        print("  âœ“ å‘é‡ç´¢å¼•ç®¡ç†å’Œæœç´¢")
        print("  âœ“ è¯­ä¹‰ç¼“å­˜ç³»ç»Ÿ")
        print("  âœ“ ä¼˜åŒ–è®°å¿†æ£€ç´¢æ¥å£")
        print("  âœ“ é«˜æ€§èƒ½å‘é‡æœç´¢")
        print("  âœ“ æ™ºèƒ½è¯­ä¹‰ç¼“å­˜")
        print("  âœ“ è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†")
        print("  âœ“ ç´¢å¼•ä¼˜åŒ–å’Œç»´æŠ¤")
        print("  âœ“ æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("  âœ“ å‘é‡ç›¸ä¼¼åº¦è®¡ç®—")
        print("  âœ“ æ•°æ®æŒä¹…åŒ–å’Œæ¢å¤")
    else:
        print(f"\nâŒ ä»»åŠ¡8.2 æµ‹è¯•å¤±è´¥ ({success_count}/{len(tests)} é€šè¿‡)")