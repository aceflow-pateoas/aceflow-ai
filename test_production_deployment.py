#!/usr/bin/env python3
"""
ç”Ÿäº§éƒ¨ç½²æµ‹è¯•å¥—ä»¶
Production Deployment Test Suite

åœ¨çœŸå®MCPç¯å¢ƒä¸­è¿›è¡Œå…¨é¢çš„éƒ¨ç½²æµ‹è¯•å’ŒéªŒè¯
"""

import asyncio
import json
import sys
import os
import time
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional
import concurrent.futures

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "aceflow-mcp-server"))

from aceflow_mcp_server.unified_server import create_unified_server
from aceflow_mcp_server.unified_config import UnifiedConfig

class ProductionDeploymentTester:
    """ç”Ÿäº§éƒ¨ç½²æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.performance_metrics = {}
        self.deployment_configs = []
        
    async def run_comprehensive_deployment_tests(self):
        """è¿è¡Œå…¨é¢çš„éƒ¨ç½²æµ‹è¯•"""
        print("ğŸš€ Starting Comprehensive Production Deployment Tests")
        print("=" * 60)
        
        # æµ‹è¯•å¥—ä»¶
        test_suites = [
            ("åŸºç¡€éƒ¨ç½²æµ‹è¯•", self.test_basic_deployment),
            ("å¤šæ¨¡å¼éƒ¨ç½²æµ‹è¯•", self.test_multi_mode_deployment),
            ("æ€§èƒ½åŸºå‡†æµ‹è¯•", self.test_performance_benchmarks),
            ("å¹¶å‘è´Ÿè½½æµ‹è¯•", self.test_concurrent_load),
            ("æ•…éšœæ¢å¤æµ‹è¯•", self.test_failure_recovery),
            ("é…ç½®çƒ­é‡è½½æµ‹è¯•", self.test_config_hot_reload),
            ("å†…å­˜æ³„æ¼æµ‹è¯•", self.test_memory_leak),
            ("é•¿æœŸç¨³å®šæ€§æµ‹è¯•", self.test_long_term_stability),
        ]
        
        for suite_name, test_func in test_suites:
            print(f"\nğŸ§ª {suite_name}...")
            try:
                start_time = time.time()
                result = await test_func()
                duration = time.time() - start_time
                
                self.test_results.append({
                    "suite": suite_name,
                    "status": "PASS" if result else "FAIL",
                    "duration": duration,
                    "details": result if isinstance(result, dict) else {}
                })
                
                status_icon = "âœ…" if result else "âŒ"
                print(f"  {status_icon} {suite_name}: {'PASS' if result else 'FAIL'} ({duration:.2f}s)")
                
            except Exception as e:
                print(f"  âŒ {suite_name}: ERROR - {e}")
                self.test_results.append({
                    "suite": suite_name,
                    "status": "ERROR",
                    "duration": 0,
                    "error": str(e)
                })
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        await self.generate_deployment_report()
    
    async def test_basic_deployment(self):
        """æµ‹è¯•åŸºç¡€éƒ¨ç½²åŠŸèƒ½"""
        try:
            # æµ‹è¯•æ‰€æœ‰ä¸‰ç§æ¨¡å¼çš„åŸºç¡€éƒ¨ç½²
            modes = ["basic", "standard", "enhanced"]
            results = {}
            
            for mode in modes:
                print(f"    ğŸ” Testing {mode} mode deployment...")
                
                # åˆ›å»ºæœåŠ¡å™¨
                server = await create_unified_server(
                    runtime_overrides={"mode": mode}
                )
                
                # åˆå§‹åŒ–æœåŠ¡å™¨
                init_success = await server.initialize()
                if not init_success:
                    results[mode] = {"status": "FAIL", "reason": "Initialization failed"}
                    continue
                
                # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
                status = server.get_server_status()
                if not status["initialized"]:
                    results[mode] = {"status": "FAIL", "reason": "Server not initialized"}
                    continue
                
                # æ£€æŸ¥æ¨¡å—åŠ è½½
                modules = server.module_manager.list_modules()
                expected_modules = ["core"]
                if mode == "enhanced":
                    expected_modules.extend(["collaboration", "intelligence"])
                
                missing_modules = [m for m in expected_modules if m not in modules]
                if missing_modules:
                    results[mode] = {
                        "status": "PARTIAL", 
                        "reason": f"Missing modules: {missing_modules}"
                    }
                else:
                    results[mode] = {"status": "PASS", "modules": modules}
                
                # æ¸…ç†
                await server.stop()
            
            # è¯„ä¼°ç»“æœ
            passed = sum(1 for r in results.values() if r["status"] == "PASS")
            total = len(results)
            
            return {
                "success": passed == total,
                "passed": passed,
                "total": total,
                "results": results
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_multi_mode_deployment(self):
        """æµ‹è¯•å¤šæ¨¡å¼éƒ¨ç½²å’Œåˆ‡æ¢"""
        try:
            print("    ğŸ” Testing mode switching...")
            
            # æµ‹è¯•æ¨¡å¼åˆ‡æ¢åºåˆ—
            mode_sequence = ["basic", "standard", "enhanced", "basic"]
            servers = []
            
            for i, mode in enumerate(mode_sequence):
                print(f"      â†’ Switching to {mode} mode...")
                
                server = await create_unified_server(
                    runtime_overrides={"mode": mode}
                )
                await server.initialize()
                
                # éªŒè¯æ¨¡å¼
                status = server.get_server_status()
                actual_mode = status["config"]["mode"]
                
                if actual_mode != mode:
                    return {
                        "success": False,
                        "error": f"Mode mismatch: expected {mode}, got {actual_mode}"
                    }
                
                servers.append(server)
            
            # æ¸…ç†æ‰€æœ‰æœåŠ¡å™¨
            for server in servers:
                await server.stop()
            
            return {
                "success": True,
                "modes_tested": len(mode_sequence),
                "sequence": mode_sequence
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_performance_benchmarks(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        try:
            print("    ğŸ” Running performance benchmarks...")
            
            # åˆ›å»ºå¢å¼ºæ¨¡å¼æœåŠ¡å™¨ï¼ˆæœ€å®Œæ•´åŠŸèƒ½ï¼‰
            server = await create_unified_server(
                runtime_overrides={"mode": "enhanced"}
            )
            await server.initialize()
            
            benchmarks = {}
            
            # 1. å¯åŠ¨æ—¶é—´åŸºå‡†
            start_time = time.time()
            test_server = await create_unified_server(
                runtime_overrides={"mode": "enhanced"}
            )
            await test_server.initialize()
            startup_time = time.time() - start_time
            await test_server.stop()
            
            benchmarks["startup_time"] = startup_time
            
            # 2. å·¥å…·è°ƒç”¨æ€§èƒ½åŸºå‡†
            core_module = server.module_manager.get_module("core")
            if core_module:
                # æµ‹è¯• aceflow_init æ€§èƒ½
                start_time = time.time()
                for _ in range(10):
                    result = core_module.aceflow_init(
                        mode="standard",
                        project_name=f"benchmark-test-{_}"
                    )
                init_avg_time = (time.time() - start_time) / 10
                benchmarks["aceflow_init_avg"] = init_avg_time
                
                # æµ‹è¯• aceflow_validate æ€§èƒ½
                start_time = time.time()
                for _ in range(10):
                    result = core_module.aceflow_validate(
                        mode="basic",
                        target="project"
                    )
                validate_avg_time = (time.time() - start_time) / 10
                benchmarks["aceflow_validate_avg"] = validate_avg_time
            
            # 3. å†…å­˜ä½¿ç”¨åŸºå‡†
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            benchmarks["memory_usage_mb"] = memory_info.rss / 1024 / 1024
            
            await server.stop()
            
            # è¯„ä¼°æ€§èƒ½
            performance_ok = (
                benchmarks["startup_time"] < 10.0 and  # å¯åŠ¨æ—¶é—´ < 10ç§’
                benchmarks.get("aceflow_init_avg", 0) < 2.0 and  # å·¥å…·è°ƒç”¨ < 2ç§’
                benchmarks["memory_usage_mb"] < 500  # å†…å­˜ä½¿ç”¨ < 500MB
            )
            
            return {
                "success": performance_ok,
                "benchmarks": benchmarks,
                "performance_grade": "EXCELLENT" if performance_ok else "NEEDS_IMPROVEMENT"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_concurrent_load(self):
        """æµ‹è¯•å¹¶å‘è´Ÿè½½"""
        try:
            print("    ğŸ” Testing concurrent load handling...")
            
            server = await create_unified_server(
                runtime_overrides={"mode": "enhanced"}
            )
            await server.initialize()
            
            # å¹¶å‘æµ‹è¯•å‚æ•°
            concurrent_requests = 20
            requests_per_worker = 5
            
            async def worker_task(worker_id):
                """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
                results = []
                core_module = server.module_manager.get_module("core")
                
                for i in range(requests_per_worker):
                    try:
                        start_time = time.time()
                        result = core_module.aceflow_init(
                            mode="standard",
                            project_name=f"concurrent-test-{worker_id}-{i}"
                        )
                        duration = time.time() - start_time
                        
                        results.append({
                            "success": True,
                            "duration": duration,
                            "worker": worker_id,
                            "request": i
                        })
                    except Exception as e:
                        results.append({
                            "success": False,
                            "error": str(e),
                            "worker": worker_id,
                            "request": i
                        })
                
                return results
            
            # å¯åŠ¨å¹¶å‘ä»»åŠ¡
            start_time = time.time()
            tasks = [worker_task(i) for i in range(concurrent_requests)]
            all_results = await asyncio.gather(*tasks)
            total_duration = time.time() - start_time
            
            # åˆ†æç»“æœ
            flat_results = [item for sublist in all_results for item in sublist]
            successful_requests = sum(1 for r in flat_results if r["success"])
            total_requests = len(flat_results)
            avg_response_time = sum(r.get("duration", 0) for r in flat_results if r["success"]) / max(successful_requests, 1)
            
            await server.stop()
            
            success_rate = successful_requests / total_requests
            load_test_passed = success_rate >= 0.95 and avg_response_time < 5.0
            
            return {
                "success": load_test_passed,
                "total_requests": total_requests,
                "successful_requests": successful_requests,
                "success_rate": success_rate,
                "avg_response_time": avg_response_time,
                "total_duration": total_duration,
                "concurrent_workers": concurrent_requests
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_failure_recovery(self):
        """æµ‹è¯•æ•…éšœæ¢å¤èƒ½åŠ›"""
        try:
            print("    ğŸ” Testing failure recovery...")
            
            server = await create_unified_server(
                runtime_overrides={"mode": "enhanced"}
            )
            await server.initialize()
            
            recovery_tests = {}
            
            # 1. æµ‹è¯•æ¨¡å—æ•…éšœæ¢å¤
            try:
                # æ¨¡æ‹Ÿæ¨¡å—æ•…éšœ
                collaboration_module = server.module_manager.get_module("collaboration")
                if collaboration_module:
                    # å¼ºåˆ¶è®¾ç½®æ¨¡å—ä¸ºæœªåˆå§‹åŒ–çŠ¶æ€
                    collaboration_module._initialized = False
                    
                    # å°è¯•é‡æ–°åˆå§‹åŒ–
                    recovery_success = collaboration_module.initialize()
                    recovery_tests["module_recovery"] = recovery_success
                else:
                    recovery_tests["module_recovery"] = "SKIPPED"
            except Exception as e:
                recovery_tests["module_recovery"] = f"ERROR: {e}"
            
            # 2. æµ‹è¯•é…ç½®é”™è¯¯æ¢å¤
            try:
                # åˆ›å»ºæ— æ•ˆé…ç½®
                invalid_config = UnifiedConfig(mode="invalid_mode")
                errors = invalid_config.get_validation_errors()
                recovery_tests["config_validation"] = len(errors) > 0
            except Exception as e:
                recovery_tests["config_validation"] = f"ERROR: {e}"
            
            # 3. æµ‹è¯•æœåŠ¡å™¨é‡å¯æ¢å¤
            try:
                await server.stop()
                server = await create_unified_server(
                    runtime_overrides={"mode": "enhanced"}
                )
                restart_success = await server.initialize()
                recovery_tests["server_restart"] = restart_success
            except Exception as e:
                recovery_tests["server_restart"] = f"ERROR: {e}"
            
            await server.stop()
            
            # è¯„ä¼°æ¢å¤èƒ½åŠ›
            successful_recoveries = sum(1 for r in recovery_tests.values() 
                                      if r is True or r == "SKIPPED")
            total_tests = len(recovery_tests)
            
            return {
                "success": successful_recoveries >= total_tests * 0.8,
                "recovery_tests": recovery_tests,
                "recovery_rate": successful_recoveries / total_tests
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_config_hot_reload(self):
        """æµ‹è¯•é…ç½®çƒ­é‡è½½"""
        try:
            print("    ğŸ” Testing configuration hot reload...")
            
            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                initial_config = {
                    "mode": "basic",
                    "core": {"enabled": True}
                }
                json.dump(initial_config, f)
                config_file = f.name
            
            try:
                server = await create_unified_server()
                await server.initialize()
                
                # æ£€æŸ¥åˆå§‹çŠ¶æ€
                initial_status = server.get_server_status()
                initial_mode = initial_status["config"]["mode"]
                
                # æ¨¡æ‹Ÿé…ç½®æ›´æ–°ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼Œè¿™éœ€è¦é…ç½®ç®¡ç†å™¨æ”¯æŒï¼‰
                # è¿™é‡Œæˆ‘ä»¬æµ‹è¯•é…ç½®ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½
                config_manager = server.config_manager
                
                # æµ‹è¯•é…ç½®æ›´æ–°
                updated_config = config_manager.get_config()
                hot_reload_supported = hasattr(config_manager, 'reload_config')
                
                await server.stop()
                
                return {
                    "success": True,
                    "initial_mode": initial_mode,
                    "hot_reload_supported": hot_reload_supported,
                    "config_manager_available": config_manager is not None
                }
                
            finally:
                os.unlink(config_file)
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_memory_leak(self):
        """æµ‹è¯•å†…å­˜æ³„æ¼"""
        try:
            print("    ğŸ” Testing for memory leaks...")
            
            import psutil
            
            # åŸºçº¿å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ‰§è¡Œå¤šæ¬¡æ“ä½œ
            for cycle in range(5):
                server = await create_unified_server(
                    runtime_overrides={"mode": "enhanced"}
                )
                await server.initialize()
                
                # æ‰§è¡Œä¸€äº›æ“ä½œ
                core_module = server.module_manager.get_module("core")
                if core_module:
                    for i in range(10):
                        result = core_module.aceflow_init(
                            mode="standard",
                            project_name=f"memory-test-{cycle}-{i}"
                        )
                
                await server.stop()
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
            
            # æ£€æŸ¥æœ€ç»ˆå†…å­˜ä½¿ç”¨
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - baseline_memory
            
            # å†…å­˜å¢é•¿è¶…è¿‡100MBè®¤ä¸ºå¯èƒ½æœ‰æ³„æ¼
            memory_leak_detected = memory_increase > 100
            
            return {
                "success": not memory_leak_detected,
                "baseline_memory_mb": baseline_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "leak_detected": memory_leak_detected
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_long_term_stability(self):
        """æµ‹è¯•é•¿æœŸç¨³å®šæ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            print("    ğŸ” Testing long-term stability (abbreviated)...")
            
            server = await create_unified_server(
                runtime_overrides={"mode": "enhanced"}
            )
            await server.initialize()
            
            # æ¨¡æ‹Ÿé•¿æœŸè¿è¡Œï¼ˆç®€åŒ–ä¸ºçŸ­æœŸé«˜é¢‘æ“ä½œï¼‰
            operations_count = 100
            errors = []
            
            core_module = server.module_manager.get_module("core")
            
            for i in range(operations_count):
                try:
                    # äº¤æ›¿æ‰§è¡Œä¸åŒæ“ä½œ
                    if i % 3 == 0:
                        result = core_module.aceflow_init(
                            mode="standard",
                            project_name=f"stability-test-{i}"
                        )
                    elif i % 3 == 1:
                        result = core_module.aceflow_stage(
                            action="status"
                        )
                    else:
                        result = core_module.aceflow_validate(
                            mode="basic",
                            target="project"
                        )
                    
                    # çŸ­æš‚å»¶è¿Ÿæ¨¡æ‹ŸçœŸå®ä½¿ç”¨
                    await asyncio.sleep(0.01)
                    
                except Exception as e:
                    errors.append(f"Operation {i}: {e}")
            
            await server.stop()
            
            error_rate = len(errors) / operations_count
            stability_ok = error_rate < 0.05  # é”™è¯¯ç‡ä½äº5%
            
            return {
                "success": stability_ok,
                "operations_completed": operations_count,
                "errors_count": len(errors),
                "error_rate": error_rate,
                "errors": errors[:5] if errors else []  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def generate_deployment_report(self):
        """ç”Ÿæˆéƒ¨ç½²æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š PRODUCTION DEPLOYMENT TEST REPORT")
        print("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r["status"] == "PASS"])
        failed_tests = len([r for r in self.test_results if r["status"] == "FAIL"])
        error_tests = len([r for r in self.test_results if r["status"] == "ERROR"])
        
        print(f"Total Test Suites: {total_tests}")
        print(f"âœ… Passed: {passed_tests}")
        print(f"âŒ Failed: {failed_tests}")
        print(f"ğŸ’¥ Errors: {error_tests}")
        print(f"Success Rate: {(passed_tests/total_tests)*100:.1f}%")
        
        print("\nğŸ“‹ Detailed Results:")
        for result in self.test_results:
            status_icon = {"PASS": "âœ…", "FAIL": "âŒ", "ERROR": "ğŸ’¥"}[result["status"]]
            duration = result.get("duration", 0)
            print(f"  {status_icon} {result['suite']}: {result['status']} ({duration:.2f}s)")
            
            if result["status"] == "ERROR" and "error" in result:
                print(f"    Error: {result['error']}")
            elif "details" in result and result["details"]:
                if isinstance(result["details"], dict) and "benchmarks" in result["details"]:
                    benchmarks = result["details"]["benchmarks"]
                    print(f"    Startup: {benchmarks.get('startup_time', 0):.2f}s, "
                          f"Memory: {benchmarks.get('memory_usage_mb', 0):.1f}MB")
        
        # éƒ¨ç½²å°±ç»ªè¯„ä¼°
        print("\nğŸ¯ Deployment Readiness Assessment:")
        if passed_tests >= total_tests * 0.8:
            print("ğŸ‰ READY FOR PRODUCTION: All critical tests passed!")
            deployment_status = "PRODUCTION_READY"
        elif passed_tests >= total_tests * 0.6:
            print("âš ï¸ READY WITH CAUTION: Most tests passed, monitor closely")
            deployment_status = "READY_WITH_MONITORING"
        else:
            print("âŒ NOT READY: Significant issues need resolution")
            deployment_status = "NOT_READY"
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            "timestamp": time.time(),
            "deployment_status": deployment_status,
            "summary": {
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "errors": error_tests,
                "success_rate": (passed_tests/total_tests)*100
            },
            "test_results": self.test_results,
            "recommendations": self.generate_recommendations()
        }
        
        with open("production_deployment_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ Detailed report saved to: production_deployment_report.json")
        
        return deployment_status == "PRODUCTION_READY"
    
    def generate_recommendations(self):
        """ç”Ÿæˆéƒ¨ç½²å»ºè®®"""
        recommendations = []
        
        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        for result in self.test_results:
            if result["status"] == "FAIL":
                if "performance" in result["suite"].lower():
                    recommendations.append("Consider performance optimization before production deployment")
                elif "concurrent" in result["suite"].lower():
                    recommendations.append("Review concurrent request handling and consider load balancing")
                elif "memory" in result["suite"].lower():
                    recommendations.append("Investigate memory usage patterns and implement monitoring")
            elif result["status"] == "ERROR":
                recommendations.append(f"Fix critical error in {result['suite']} before deployment")
        
        if not recommendations:
            recommendations.append("All tests passed - system is ready for production deployment")
        
        return recommendations

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ AceFlow MCP Server - Production Deployment Test Suite")
    print("=" * 60)
    
    tester = ProductionDeploymentTester()
    
    try:
        success = await tester.run_comprehensive_deployment_tests()
        return 0 if success else 1
        
    except Exception as e:
        print(f"âŒ Deployment test suite failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)