"""
ä¼˜åŒ–çš„çŠ¶æ€ç®¡ç†å™¨
å®ç°LRUç¼“å­˜ã€å¼‚æ­¥å¤„ç†å’Œå†…å­˜ç´¢å¼•çš„é«˜æ€§èƒ½çŠ¶æ€ç®¡ç†
"""

import asyncio
import json
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
import hashlib

from .models import PATEOASState, StateTransition, MemoryFragment
from .config import get_config
from .utils import generate_id, ensure_directory


class LRUCache:
    """LRUç¼“å­˜å®ç°"""
    
    def __init__(self, capacity: int = 1000):
        self.capacity = capacity
        self.cache = OrderedDict()
        self.access_times = {}
        self.hit_count = 0
        self.miss_count = 0
        self._lock = threading.RLock()
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜é¡¹"""
        with self._lock:
            if key in self.cache:
                # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                value = self.cache.pop(key)
                self.cache[key] = value
                self.access_times[key] = time.time()
                self.hit_count += 1
                return value
            else:
                self.miss_count += 1
                return None
    
    def put(self, key: str, value: Any) -> None:
        """æ·»åŠ ç¼“å­˜é¡¹"""
        with self._lock:
            if key in self.cache:
                # æ›´æ–°ç°æœ‰é¡¹
                self.cache.pop(key)
            elif len(self.cache) >= self.capacity:
                # ç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
                oldest_key = next(iter(self.cache))
                self.cache.pop(oldest_key)
                self.access_times.pop(oldest_key, None)
            
            self.cache[key] = value
            self.access_times[key] = time.time()
    
    def remove(self, key: str) -> bool:
        """ç§»é™¤ç¼“å­˜é¡¹"""
        with self._lock:
            if key in self.cache:
                self.cache.pop(key)
                self.access_times.pop(key, None)
                return True
            return False
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.access_times.clear()
            self.hit_count = 0
            self.miss_count = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        with self._lock:
            total_requests = self.hit_count + self.miss_count
            hit_rate = self.hit_count / max(1, total_requests)
            
            return {
                'capacity': self.capacity,
                'size': len(self.cache),
                'hit_count': self.hit_count,
                'miss_count': self.miss_count,
                'hit_rate': hit_rate,
                'total_requests': total_requests
            }


class StateIndex:
    """çŠ¶æ€ç´¢å¼•ï¼Œç”¨äºå¿«é€Ÿæ£€ç´¢"""
    
    def __init__(self):
        self.project_index = {}  # project_id -> state_keys
        self.timestamp_index = {}  # timestamp -> state_keys
        self.tag_index = {}  # tag -> state_keys
        self.content_hash_index = {}  # content_hash -> state_key
        self._lock = threading.RLock()
    
    def add_state(self, state_key: str, project_id: str, timestamp: datetime, 
                  tags: List[str] = None, content_hash: str = None):
        """æ·»åŠ çŠ¶æ€åˆ°ç´¢å¼•"""
        with self._lock:
            # é¡¹ç›®ç´¢å¼•
            if project_id not in self.project_index:
                self.project_index[project_id] = set()
            self.project_index[project_id].add(state_key)
            
            # æ—¶é—´æˆ³ç´¢å¼•ï¼ˆæŒ‰å°æ—¶åˆ†ç»„ï¼‰
            hour_key = timestamp.strftime('%Y-%m-%d-%H')
            if hour_key not in self.timestamp_index:
                self.timestamp_index[hour_key] = set()
            self.timestamp_index[hour_key].add(state_key)
            
            # æ ‡ç­¾ç´¢å¼•
            if tags:
                for tag in tags:
                    if tag not in self.tag_index:
                        self.tag_index[tag] = set()
                    self.tag_index[tag].add(state_key)
            
            # å†…å®¹å“ˆå¸Œç´¢å¼•
            if content_hash:
                self.content_hash_index[content_hash] = state_key
    
    def find_by_project(self, project_id: str) -> List[str]:
        """æ ¹æ®é¡¹ç›®IDæŸ¥æ‰¾çŠ¶æ€"""
        with self._lock:
            return list(self.project_index.get(project_id, set()))
    
    def find_by_timerange(self, start_time: datetime, end_time: datetime) -> List[str]:
        """æ ¹æ®æ—¶é—´èŒƒå›´æŸ¥æ‰¾çŠ¶æ€"""
        with self._lock:
            result = set()
            current = start_time
            
            while current <= end_time:
                hour_key = current.strftime('%Y-%m-%d-%H')
                if hour_key in self.timestamp_index:
                    result.update(self.timestamp_index[hour_key])
                current += timedelta(hours=1)
            
            return list(result)
    
    def find_by_tags(self, tags: List[str]) -> List[str]:
        """æ ¹æ®æ ‡ç­¾æŸ¥æ‰¾çŠ¶æ€"""
        with self._lock:
            if not tags:
                return []
            
            result = self.tag_index.get(tags[0], set())
            for tag in tags[1:]:
                if tag in self.tag_index:
                    result = result.intersection(self.tag_index[tag])
                else:
                    return []
            
            return list(result)
    
    def find_by_content_hash(self, content_hash: str) -> Optional[str]:
        """æ ¹æ®å†…å®¹å“ˆå¸ŒæŸ¥æ‰¾çŠ¶æ€"""
        with self._lock:
            return self.content_hash_index.get(content_hash)
    
    def remove_state(self, state_key: str):
        """ä»ç´¢å¼•ä¸­ç§»é™¤çŠ¶æ€"""
        with self._lock:
            # ä»æ‰€æœ‰ç´¢å¼•ä¸­ç§»é™¤
            for project_states in self.project_index.values():
                project_states.discard(state_key)
            
            for timestamp_states in self.timestamp_index.values():
                timestamp_states.discard(state_key)
            
            for tag_states in self.tag_index.values():
                tag_states.discard(state_key)
            
            # ä»å†…å®¹å“ˆå¸Œç´¢å¼•ä¸­ç§»é™¤
            to_remove = []
            for hash_key, key in self.content_hash_index.items():
                if key == state_key:
                    to_remove.append(hash_key)
            
            for hash_key in to_remove:
                self.content_hash_index.pop(hash_key)


class AsyncStateProcessor:
    """å¼‚æ­¥çŠ¶æ€å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.pending_operations = {}
        self._lock = threading.RLock()
    
    async def process_state_async(self, operation: str, state_data: Dict[str, Any], 
                                  callback: Optional[callable] = None) -> Any:
        """å¼‚æ­¥å¤„ç†çŠ¶æ€æ“ä½œ"""
        loop = asyncio.get_event_loop()
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒçŠ¶æ€æ“ä½œ
        future = loop.run_in_executor(
            self.executor, 
            self._execute_state_operation, 
            operation, 
            state_data
        )
        
        # è®°å½•å¾…å¤„ç†æ“ä½œ
        operation_id = generate_id()
        with self._lock:
            self.pending_operations[operation_id] = {
                'operation': operation,
                'start_time': time.time(),
                'future': future
            }
        
        try:
            result = await future
            
            # æ‰§è¡Œå›è°ƒ
            if callback:
                callback(result)
            
            return result
        
        finally:
            # æ¸…ç†å¾…å¤„ç†æ“ä½œ
            with self._lock:
                self.pending_operations.pop(operation_id, None)
    
    def _execute_state_operation(self, operation: str, state_data: Dict[str, Any]) -> Any:
        """æ‰§è¡ŒçŠ¶æ€æ“ä½œ"""
        if operation == 'serialize':
            return self._serialize_state(state_data)
        elif operation == 'deserialize':
            return self._deserialize_state(state_data)
        elif operation == 'validate':
            return self._validate_state(state_data)
        elif operation == 'compress':
            return self._compress_state(state_data)
        elif operation == 'decompress':
            return self._decompress_state(state_data)
        else:
            raise ValueError(f"Unknown operation: {operation}")
    
    def _serialize_state(self, state_data: Dict[str, Any]) -> str:
        """åºåˆ—åŒ–çŠ¶æ€æ•°æ®"""
        return json.dumps(state_data, ensure_ascii=False, separators=(',', ':'))
    
    def _deserialize_state(self, serialized_data: Dict[str, Any]) -> Dict[str, Any]:
        """ååºåˆ—åŒ–çŠ¶æ€æ•°æ®"""
        if isinstance(serialized_data, str):
            return json.loads(serialized_data)
        return serialized_data
    
    def _validate_state(self, state_data: Dict[str, Any]) -> bool:
        """éªŒè¯çŠ¶æ€æ•°æ®"""
        required_fields = ['project_id', 'timestamp', 'workflow_state']
        return all(field in state_data for field in required_fields)
    
    def _compress_state(self, state_data: Dict[str, Any]) -> Dict[str, Any]:
        """å‹ç¼©çŠ¶æ€æ•°æ®ï¼ˆç§»é™¤å†—ä½™ä¿¡æ¯ï¼‰"""
        compressed = state_data.copy()
        
        # ç§»é™¤ç©ºå€¼å’Œé»˜è®¤å€¼
        if 'execution_history' in compressed and not compressed['execution_history']:
            compressed.pop('execution_history')
        
        if 'ai_memory' in compressed and not compressed['ai_memory']:
            compressed.pop('ai_memory')
        
        return compressed
    
    def _decompress_state(self, compressed_data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£å‹ç¼©çŠ¶æ€æ•°æ®ï¼ˆæ¢å¤é»˜è®¤å€¼ï¼‰"""
        decompressed = compressed_data.copy()
        
        # æ¢å¤é»˜è®¤å€¼
        if 'execution_history' not in decompressed:
            decompressed['execution_history'] = []
        
        if 'ai_memory' not in decompressed:
            decompressed['ai_memory'] = []
        
        return decompressed
    
    def get_pending_operations(self) -> Dict[str, Any]:
        """è·å–å¾…å¤„ç†æ“ä½œç»Ÿè®¡"""
        with self._lock:
            return {
                'count': len(self.pending_operations),
                'operations': [
                    {
                        'id': op_id,
                        'operation': op_info['operation'],
                        'duration': time.time() - op_info['start_time']
                    }
                    for op_id, op_info in self.pending_operations.items()
                ]
            }
    
    def shutdown(self):
        """å…³é—­å¼‚æ­¥å¤„ç†å™¨"""
        self.executor.shutdown(wait=True)


class OptimizedStateManager:
    """ä¼˜åŒ–çš„çŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self, project_id: str = "default", cache_size: int = 1000):
        self.project_id = project_id
        self.config = get_config()
        
        # æ ¸å¿ƒç»„ä»¶
        self.cache = LRUCache(capacity=cache_size)
        self.index = StateIndex()
        self.async_processor = AsyncStateProcessor()
        
        # çŠ¶æ€å­˜å‚¨
        self.current_state: Optional[Dict[str, Any]] = None
        self.state_history: List[Dict[str, Any]] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'async_operations': 0,
            'sync_operations': 0,
            'average_operation_time': 0.0,
            'total_operations': 0
        }
        
        # å­˜å‚¨é…ç½®
        self.state_dir = ensure_directory(Path(self.config.state_storage_path) / "optimized")
        self.state_file = self.state_dir / f"{project_id}_optimized_state.json"
        self.index_file = self.state_dir / f"{project_id}_state_index.json"
        
        # åˆå§‹åŒ–
        self._load_state_and_index()
        
        print(f"âœ“ ä¼˜åŒ–çŠ¶æ€ç®¡ç†å™¨å·²åˆå§‹åŒ– (é¡¹ç›®: {project_id}, ç¼“å­˜å®¹é‡: {cache_size})")
    
    def get_current_state(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        start_time = time.time()
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cache_key = f"current_state_{self.project_id}"
        cached_state = self.cache.get(cache_key)
        
        if cached_state:
            self.performance_stats['cache_hits'] += 1
            self._update_performance_stats(time.time() - start_time)
            return cached_state
        
        self.performance_stats['cache_misses'] += 1
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ„å»ºçŠ¶æ€
        if not self.current_state:
            self._initialize_default_state()
        
        state = {
            'project_id': self.project_id,
            'timestamp': datetime.now().isoformat(),
            'project_context': self._get_project_context(),
            'workflow_state': self._get_workflow_state(),
            'ai_memory': self._get_ai_memory(),
            'user_preferences': self._get_user_preferences(),
            'execution_history': self._get_execution_history(),
            'performance_metrics': self._get_performance_metrics()
        }
        
        # æ·»åŠ åˆ°ç¼“å­˜
        self.cache.put(cache_key, state)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        content_hash = self._calculate_content_hash(state)
        self.index.add_state(
            state_key=cache_key,
            project_id=self.project_id,
            timestamp=datetime.now(),
            tags=['current', 'active'],
            content_hash=content_hash
        )
        
        self._update_performance_stats(time.time() - start_time)
        return state
    
    async def get_current_state_async(self) -> Dict[str, Any]:
        """å¼‚æ­¥è·å–å½“å‰çŠ¶æ€"""
        self.performance_stats['async_operations'] += 1
        
        # å¼‚æ­¥å¤„ç†çŠ¶æ€æ„å»º
        state_data = {'project_id': self.project_id}
        
        processed_state = await self.async_processor.process_state_async(
            operation='deserialize',
            state_data=state_data
        )
        
        return self.get_current_state()
    
    def update_state(self, new_information: Dict[str, Any], async_mode: bool = False):
        """æ›´æ–°çŠ¶æ€ï¼ˆæ”¯æŒå¼‚æ­¥æ¨¡å¼ï¼‰"""
        start_time = time.time()
        
        if async_mode:
            return self._update_state_async(new_information)
        else:
            return self._update_state_sync(new_information, start_time)
    
    def _update_state_sync(self, new_information: Dict[str, Any], start_time: float):
        """åŒæ­¥æ›´æ–°çŠ¶æ€"""
        self.performance_stats['sync_operations'] += 1
        
        if not self.current_state:
            self._initialize_default_state()
        
        # è®°å½•çŠ¶æ€å˜åŒ–
        old_state = self.current_state.copy() if self.current_state else {}
        
        # æ›´æ–°çŠ¶æ€
        self._merge_state_information(new_information)
        
        # è®°å½•çŠ¶æ€è½¬æ¢
        transition = {
            'timestamp': datetime.now().isoformat(),
            'changes': self._calculate_state_changes(old_state, self.current_state),
            'trigger': new_information.get('trigger', 'manual_update')
        }
        
        # æ·»åŠ åˆ°å†å²
        self.state_history.append(transition)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.state_history) > 100:
            self.state_history = self.state_history[-50:]
        
        # æ›´æ–°ç¼“å­˜ - ç§»é™¤æ—§ç¼“å­˜ï¼Œä¸‹æ¬¡è·å–æ—¶ä¼šé‡æ–°æ„å»º
        cache_key = f"current_state_{self.project_id}"
        self.cache.remove(cache_key)  # ç§»é™¤æ—§ç¼“å­˜
        
        # ä¿å­˜çŠ¶æ€
        self._save_state()
        
        self._update_performance_stats(time.time() - start_time)
    
    async def _update_state_async(self, new_information: Dict[str, Any]):
        """å¼‚æ­¥æ›´æ–°çŠ¶æ€"""
        self.performance_stats['async_operations'] += 1
        
        # å¼‚æ­¥å¤„ç†çŠ¶æ€æ›´æ–°
        await self.async_processor.process_state_async(
            operation='validate',
            state_data=new_information,
            callback=lambda result: self._update_state_sync(new_information, time.time())
        )
    
    def get_state_history(self, limit: int = 10, 
                         start_time: Optional[datetime] = None,
                         end_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """è·å–çŠ¶æ€å†å²ï¼ˆæ”¯æŒæ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰"""
        start_op_time = time.time()
        
        # ä½¿ç”¨ç´¢å¼•å¿«é€ŸæŸ¥æ‰¾
        if start_time and end_time:
            state_keys = self.index.find_by_timerange(start_time, end_time)
            
            # ä»ç¼“å­˜æˆ–å­˜å‚¨ä¸­è·å–çŠ¶æ€
            history = []
            for key in state_keys[:limit]:
                cached_state = self.cache.get(key)
                if cached_state:
                    history.append(cached_state)
        else:
            # è¿”å›æœ€è¿‘çš„å†å²è®°å½•
            history = self.state_history[-limit:] if self.state_history else []
        
        self._update_performance_stats(time.time() - start_op_time)
        return history
    
    def find_similar_states(self, target_state: Dict[str, Any], 
                           similarity_threshold: float = 0.8) -> List[Tuple[str, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼çŠ¶æ€"""
        start_time = time.time()
        
        target_hash = self._calculate_content_hash(target_state)
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„çŠ¶æ€
        exact_match = self.index.find_by_content_hash(target_hash)
        if exact_match:
            self._update_performance_stats(time.time() - start_time)
            return [(exact_match, 1.0)]
        
        # æŸ¥æ‰¾ç›¸ä¼¼çŠ¶æ€ï¼ˆç®€åŒ–å®ç°ï¼‰
        similar_states = []
        
        # åŸºäºé¡¹ç›®IDæŸ¥æ‰¾å€™é€‰çŠ¶æ€
        project_states = self.index.find_by_project(self.project_id)
        
        for state_key in project_states:
            cached_state = self.cache.get(state_key)
            if cached_state:
                similarity = self._calculate_state_similarity(target_state, cached_state)
                if similarity >= similarity_threshold:
                    similar_states.append((state_key, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_states.sort(key=lambda x: x[1], reverse=True)
        
        self._update_performance_stats(time.time() - start_time)
        return similar_states[:10]  # è¿”å›å‰10ä¸ªæœ€ç›¸ä¼¼çš„çŠ¶æ€
    
    def optimize_cache(self):
        """ä¼˜åŒ–ç¼“å­˜æ€§èƒ½"""
        print("ğŸ”§ å¼€å§‹ç¼“å­˜ä¼˜åŒ–...")
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.cache.get_stats()
        print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
        print(f"  - ç¼“å­˜ä½¿ç”¨ç‡: {cache_stats['size']}/{cache_stats['capacity']}")
        
        # å¦‚æœå‘½ä¸­ç‡ä½ï¼Œå¢åŠ ç¼“å­˜å®¹é‡
        if cache_stats['hit_rate'] < 0.7 and cache_stats['size'] >= cache_stats['capacity'] * 0.9:
            new_capacity = int(cache_stats['capacity'] * 1.5)
            print(f"  - æ‰©å±•ç¼“å­˜å®¹é‡: {cache_stats['capacity']} -> {new_capacity}")
            
            # åˆ›å»ºæ–°çš„æ›´å¤§ç¼“å­˜
            old_cache = self.cache
            self.cache = LRUCache(capacity=new_capacity)
            
            # è¿ç§»çƒ­ç‚¹æ•°æ®
            for key, value in old_cache.cache.items():
                self.cache.put(key, value)
        
        # æ¸…ç†è¿‡æœŸçš„ç´¢å¼•é¡¹
        self._cleanup_expired_index_items()
        
        print("âœ“ ç¼“å­˜ä¼˜åŒ–å®Œæˆ")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        cache_stats = self.cache.get_stats()
        pending_ops = self.async_processor.get_pending_operations()
        
        return {
            'cache_performance': cache_stats,
            'operation_stats': self.performance_stats,
            'async_operations': pending_ops,
            'index_stats': {
                'projects': len(self.index.project_index),
                'timestamps': len(self.index.timestamp_index),
                'tags': len(self.index.tag_index),
                'content_hashes': len(self.index.content_hash_index)
            },
            'memory_usage': {
                'state_history_size': len(self.state_history),
                'cache_size': cache_stats['size']
            }
        }
    
    def benchmark_performance(self, num_operations: int = 100) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"ğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({num_operations} æ¬¡æ“ä½œ)...")
        
        # æµ‹è¯•çŠ¶æ€è·å–æ€§èƒ½
        get_times = []
        for i in range(num_operations):
            start_time = time.time()
            self.get_current_state()
            get_times.append(time.time() - start_time)
        
        # æµ‹è¯•çŠ¶æ€æ›´æ–°æ€§èƒ½
        update_times = []
        for i in range(num_operations // 10):  # æ›´æ–°æ“ä½œè¾ƒå°‘
            start_time = time.time()
            self.update_state({
                'test_update': f'benchmark_{i}',
                'timestamp': datetime.now().isoformat()
            })
            update_times.append(time.time() - start_time)
        
        # æµ‹è¯•ç¼“å­˜æ€§èƒ½
        cache_stats = self.cache.get_stats()
        
        avg_get_time = sum(get_times) / len(get_times)
        avg_update_time = sum(update_times) / len(update_times)
        
        performance_grade = 'A' if avg_get_time < 0.001 else 'B' if avg_get_time < 0.01 else 'C'
        
        benchmark_result = {
            'average_get_time': avg_get_time,
            'average_update_time': avg_update_time,
            'operations_per_second': 1.0 / avg_get_time,
            'cache_hit_rate': cache_stats['hit_rate'],
            'performance_grade': performance_grade,
            'total_operations': num_operations,
            'cache_efficiency': cache_stats['hit_rate'] * (1.0 / max(0.001, avg_get_time))
        }
        
        print(f"âœ“ åŸºå‡†æµ‹è¯•å®Œæˆ:")
        print(f"  - å¹³å‡è·å–æ—¶é—´: {avg_get_time:.6f}s")
        print(f"  - å¹³å‡æ›´æ–°æ—¶é—´: {avg_update_time:.6f}s")
        print(f"  - æ¯ç§’æ“ä½œæ•°: {benchmark_result['operations_per_second']:.1f}")
        print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
        print(f"  - æ€§èƒ½ç­‰çº§: {performance_grade}")
        
        return benchmark_result
    
    def _initialize_default_state(self):
        """åˆå§‹åŒ–é»˜è®¤çŠ¶æ€"""
        self.current_state = {
            'project_id': self.project_id,
            'created_at': datetime.now().isoformat(),
            'workflow_state': {
                'current_stage': 'S1',
                'stage_progress': 0.0,
                'completed_stages': [],
                'active_tasks': []
            },
            'project_context': {
                'project_type': 'general',
                'complexity': 'medium',
                'team_size': 1,
                'timeline': 'flexible'
            },
            'ai_memory': [],
            'user_preferences': {},
            'execution_history': []
        }
    
    def _get_project_context(self) -> Dict[str, Any]:
        """è·å–é¡¹ç›®ä¸Šä¸‹æ–‡"""
        if self.current_state and 'project_context' in self.current_state:
            return self.current_state['project_context']
        return {
            'project_type': 'general',
            'complexity': 'medium',
            'team_size': 1,
            'timeline': 'flexible'
        }
    
    def _get_workflow_state(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        if self.current_state and 'workflow_state' in self.current_state:
            return self.current_state['workflow_state']
        return {
            'current_stage': 'S1',
            'stage_progress': 0.0,
            'completed_stages': [],
            'active_tasks': []
        }
    
    def _get_ai_memory(self) -> List[Dict[str, Any]]:
        """è·å–AIè®°å¿†"""
        if self.current_state and 'ai_memory' in self.current_state:
            return self.current_state['ai_memory']
        return []
    
    def _get_user_preferences(self) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·åå¥½"""
        if self.current_state and 'user_preferences' in self.current_state:
            return self.current_state['user_preferences']
        return {}
    
    def _get_execution_history(self) -> List[Dict[str, Any]]:
        """è·å–æ‰§è¡Œå†å²"""
        if self.current_state and 'execution_history' in self.current_state:
            return self.current_state['execution_history']
        return []
    
    def _get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return {
            'cache_hit_rate': self.cache.get_stats()['hit_rate'],
            'total_operations': self.performance_stats['total_operations'],
            'average_operation_time': self.performance_stats['average_operation_time']
        }
    
    def _merge_state_information(self, new_information: Dict[str, Any]):
        """åˆå¹¶çŠ¶æ€ä¿¡æ¯"""
        if not self.current_state:
            self._initialize_default_state()
        
        # æ·±åº¦åˆå¹¶çŠ¶æ€ä¿¡æ¯
        for key, value in new_information.items():
            if key in self.current_state and isinstance(self.current_state[key], dict) and isinstance(value, dict):
                # é€’å½’åˆå¹¶å­—å…¸
                self._deep_merge_dict(self.current_state[key], value)
            else:
                self.current_state[key] = value
        
        # æ›´æ–°æ—¶é—´æˆ³
        self.current_state['last_updated'] = datetime.now().isoformat()
    
    def _deep_merge_dict(self, target: Dict[str, Any], source: Dict[str, Any]):
        """æ·±åº¦åˆå¹¶å­—å…¸"""
        for key, value in source.items():
            if key in target and isinstance(target[key], dict) and isinstance(value, dict):
                self._deep_merge_dict(target[key], value)
            else:
                target[key] = value
    
    def _calculate_state_changes(self, old_state: Dict[str, Any], new_state: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è®¡ç®—çŠ¶æ€å˜åŒ–"""
        changes = []
        
        # ç®€åŒ–çš„å˜åŒ–æ£€æµ‹
        for key, new_value in new_state.items():
            if key not in old_state:
                changes.append({
                    'type': 'added',
                    'field': key,
                    'new_value': new_value
                })
            elif old_state[key] != new_value:
                changes.append({
                    'type': 'modified',
                    'field': key,
                    'old_value': old_state[key],
                    'new_value': new_value
                })
        
        for key in old_state:
            if key not in new_state:
                changes.append({
                    'type': 'removed',
                    'field': key,
                    'old_value': old_state[key]
                })
        
        return changes
    
    def _calculate_content_hash(self, state: Dict[str, Any]) -> str:
        """è®¡ç®—çŠ¶æ€å†…å®¹å“ˆå¸Œ"""
        # ç§»é™¤æ—¶é—´æˆ³ç­‰å˜åŒ–å­—æ®µ
        hashable_state = state.copy()
        hashable_state.pop('timestamp', None)
        hashable_state.pop('last_updated', None)
        
        content_str = json.dumps(hashable_state, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content_str.encode()).hexdigest()
    
    def _calculate_state_similarity(self, state1: Dict[str, Any], state2: Dict[str, Any]) -> float:
        """è®¡ç®—çŠ¶æ€ç›¸ä¼¼åº¦"""
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        common_keys = set(state1.keys()) & set(state2.keys())
        if not common_keys:
            return 0.0
        
        matching_values = 0
        for key in common_keys:
            if state1[key] == state2[key]:
                matching_values += 1
        
        return matching_values / len(common_keys)
    
    def _update_performance_stats(self, operation_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats['total_operations'] += 1
        
        # æ›´æ–°å¹³å‡æ“ä½œæ—¶é—´
        total_ops = self.performance_stats['total_operations']
        current_avg = self.performance_stats['average_operation_time']
        
        self.performance_stats['average_operation_time'] = (
            (current_avg * (total_ops - 1) + operation_time) / total_ops
        )
    
    def _cleanup_expired_index_items(self):
        """æ¸…ç†è¿‡æœŸçš„ç´¢å¼•é¡¹"""
        # æ¸…ç†è¶…è¿‡24å°æ—¶çš„æ—¶é—´æˆ³ç´¢å¼•
        cutoff_time = datetime.now() - timedelta(hours=24)
        expired_hours = []
        
        for hour_key in self.index.timestamp_index.keys():
            try:
                hour_time = datetime.strptime(hour_key, '%Y-%m-%d-%H')
                if hour_time < cutoff_time:
                    expired_hours.append(hour_key)
            except ValueError:
                expired_hours.append(hour_key)  # æ— æ•ˆæ ¼å¼ä¹Ÿæ¸…ç†
        
        for hour_key in expired_hours:
            self.index.timestamp_index.pop(hour_key, None)
    
    def _load_state_and_index(self):
        """åŠ è½½çŠ¶æ€å’Œç´¢å¼•"""
        try:
            # åŠ è½½çŠ¶æ€
            if self.state_file.exists():
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.current_state = data.get('current_state')
                    self.state_history = data.get('state_history', [])
                    self.performance_stats.update(data.get('performance_stats', {}))
            
            # åŠ è½½ç´¢å¼•
            if self.index_file.exists():
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                    # é‡å»ºç´¢å¼•ï¼Œå°†åˆ—è¡¨è½¬æ¢ä¸ºé›†åˆ
                    project_index_data = index_data.get('project_index', {})
                    self.index.project_index = {k: set(v) for k, v in project_index_data.items()}
                    
                    tag_index_data = index_data.get('tag_index', {})
                    self.index.tag_index = {k: set(v) for k, v in tag_index_data.items()}
                    
        except Exception as e:
            print(f"âš ï¸ åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
            self._initialize_default_state()
    
    def _save_state(self):
        """ä¿å­˜çŠ¶æ€å’Œç´¢å¼•"""
        try:
            # ä¿å­˜çŠ¶æ€
            state_data = {
                'current_state': self.current_state,
                'state_history': self.state_history,
                'performance_stats': self.performance_stats,
                'last_saved': datetime.now().isoformat()
            }
            
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ç´¢å¼•
            index_data = {
                'project_index': {k: list(v) for k, v in self.index.project_index.items()},
                'tag_index': {k: list(v) for k, v in self.index.tag_index.items()},
                'last_saved': datetime.now().isoformat()
            }
            
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            self.async_processor.shutdown()
        except:
            pass