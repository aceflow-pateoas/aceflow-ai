"""
ä¼˜åŒ–çš„è®°å¿†æ£€ç´¢ç³»ç»Ÿ
å®ç°å‘é‡ç´¢å¼•ã€è¯­ä¹‰ç¼“å­˜å’Œé«˜æ€§èƒ½è®°å¿†æ£€ç´¢
"""

import json
import time
import hashlib
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from pathlib import Path
from collections import defaultdict, OrderedDict
import numpy as np
from dataclasses import dataclass, field

from .models import MemoryFragment, MemoryCategory
from .config import get_config
from .utils import ensure_directory, calculate_similarity


@dataclass
class VectorIndex:
    """å‘é‡ç´¢å¼•æ•°æ®ç»“æ„"""
    memory_id: str
    vector: np.ndarray
    category: str
    importance: float
    timestamp: datetime
    tags: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'memory_id': self.memory_id,
            'vector': self.vector.tolist(),
            'category': self.category,
            'importance': self.importance,
            'timestamp': self.timestamp.isoformat(),
            'tags': self.tags
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'VectorIndex':
        """ä»å­—å…¸åˆ›å»ºå‘é‡ç´¢å¼•"""
        return cls(
            memory_id=data['memory_id'],
            vector=np.array(data['vector']),
            category=data['category'],
            importance=data['importance'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            tags=data.get('tags', [])
        )


@dataclass
class SemanticCacheEntry:
    """è¯­ä¹‰ç¼“å­˜æ¡ç›®"""
    query_hash: str
    query_text: str
    results: List[Dict[str, Any]]
    timestamp: datetime
    access_count: int = 0
    last_access: Optional[datetime] = None
    
    def update_access(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.access_count += 1
        self.last_access = datetime.now()
    
    def is_expired(self, ttl_hours: int = 24) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if not self.last_access:
            return (datetime.now() - self.timestamp).total_seconds() > ttl_hours * 3600
        return (datetime.now() - self.last_access).total_seconds() > ttl_hours * 3600


class VectorIndexManager:
    """å‘é‡ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.indices: Dict[str, VectorIndex] = {}
        self.category_indices: Dict[str, Set[str]] = defaultdict(set)
        self.tag_indices: Dict[str, Set[str]] = defaultdict(set)
        self.importance_sorted: List[str] = []  # æŒ‰é‡è¦æ€§æ’åºçš„è®°å¿†ID
        self._lock = threading.RLock()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_vectors': 0,
            'search_count': 0,
            'average_search_time': 0.0,
            'cache_hits': 0,
            'index_updates': 0
        }
    
    def add_vector(self, memory_id: str, content: str, category: str, 
                   importance: float, tags: List[str] = None) -> bool:
        """æ·»åŠ å‘é‡åˆ°ç´¢å¼•"""
        with self._lock:
            try:
                # ç”Ÿæˆå‘é‡è¡¨ç¤º
                vector = self._text_to_vector(content)
                
                # åˆ›å»ºå‘é‡ç´¢å¼•
                index_entry = VectorIndex(
                    memory_id=memory_id,
                    vector=vector,
                    category=category,
                    importance=importance,
                    timestamp=datetime.now(),
                    tags=tags or []
                )
                
                # æ·»åŠ åˆ°ä¸»ç´¢å¼•
                self.indices[memory_id] = index_entry
                
                # æ›´æ–°åˆ†ç±»ç´¢å¼•
                self.category_indices[category].add(memory_id)
                
                # æ›´æ–°æ ‡ç­¾ç´¢å¼•
                for tag in (tags or []):
                    self.tag_indices[tag].add(memory_id)
                
                # æ›´æ–°é‡è¦æ€§æ’åº
                self._update_importance_sorting()
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['total_vectors'] += 1
                self.stats['index_updates'] += 1
                
                return True
                
            except Exception as e:
                print(f"æ·»åŠ å‘é‡ç´¢å¼•å¤±è´¥: {e}")
                return False
    
    def search_similar(self, query: str, limit: int = 10, 
                      category_filter: Optional[str] = None,
                      tag_filter: Optional[List[str]] = None,
                      min_similarity: float = 0.3) -> List[Tuple[str, float]]:
        """æœç´¢ç›¸ä¼¼å‘é‡"""
        start_time = time.time()
        
        with self._lock:
            try:
                # ç”ŸæˆæŸ¥è¯¢å‘é‡
                query_vector = self._text_to_vector(query)
                
                # è·å–å€™é€‰è®°å¿†ID
                candidate_ids = self._get_candidate_ids(category_filter, tag_filter)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = []
                for memory_id in candidate_ids:
                    if memory_id in self.indices:
                        index_entry = self.indices[memory_id]
                        similarity = self._cosine_similarity(query_vector, index_entry.vector)
                        
                        if similarity >= min_similarity:
                            similarities.append((memory_id, similarity))
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                similarities.sort(key=lambda x: x[1], reverse=True)
                
                # æ›´æ–°ç»Ÿè®¡
                search_time = time.time() - start_time
                self.stats['search_count'] += 1
                self._update_average_search_time(search_time)
                
                return similarities[:limit]
                
            except Exception as e:
                print(f"å‘é‡æœç´¢å¤±è´¥: {e}")
                return []
    
    def get_top_important(self, limit: int = 10, 
                         category_filter: Optional[str] = None) -> List[str]:
        """è·å–æœ€é‡è¦çš„è®°å¿†"""
        with self._lock:
            if category_filter:
                filtered_ids = [
                    mid for mid in self.importance_sorted 
                    if mid in self.indices and self.indices[mid].category == category_filter
                ]
                return filtered_ids[:limit]
            else:
                return self.importance_sorted[:limit]
    
    def remove_vector(self, memory_id: str) -> bool:
        """ç§»é™¤å‘é‡ç´¢å¼•"""
        with self._lock:
            if memory_id not in self.indices:
                return False
            
            index_entry = self.indices[memory_id]
            
            # ä»ä¸»ç´¢å¼•ç§»é™¤
            del self.indices[memory_id]
            
            # ä»åˆ†ç±»ç´¢å¼•ç§»é™¤
            self.category_indices[index_entry.category].discard(memory_id)
            
            # ä»æ ‡ç­¾ç´¢å¼•ç§»é™¤
            for tag in index_entry.tags:
                self.tag_indices[tag].discard(memory_id)
            
            # ä»é‡è¦æ€§æ’åºç§»é™¤
            if memory_id in self.importance_sorted:
                self.importance_sorted.remove(memory_id)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_vectors'] -= 1
            
            return True
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {
                **self.stats,
                'categories': len(self.category_indices),
                'tags': len(self.tag_indices),
                'dimension': self.dimension
            }
    
    def _text_to_vector(self, text: str) -> np.ndarray:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„å‘é‡åŒ–æ–¹æ³•
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„åµŒå…¥æ¨¡å‹å¦‚BERTã€Sentence-BERTç­‰
        
        # åŸºäºå­—ç¬¦é¢‘ç‡çš„ç®€å•å‘é‡åŒ–
        vector = np.zeros(self.dimension)
        
        # è®¡ç®—å­—ç¬¦é¢‘ç‡
        char_freq = {}
        for char in text.lower():
            if char.isalnum():
                char_freq[char] = char_freq.get(char, 0) + 1
        
        # å°†å­—ç¬¦é¢‘ç‡æ˜ å°„åˆ°å‘é‡ç»´åº¦
        for i, char in enumerate('abcdefghijklmnopqrstuvwxyz0123456789'):
            if i < self.dimension:
                vector[i] = char_freq.get(char, 0)
        
        # æ·»åŠ æ–‡æœ¬é•¿åº¦ç‰¹å¾
        if self.dimension > 36:
            vector[36] = len(text)
            vector[37] = len(text.split())
        
        # æ·»åŠ å…³é”®è¯ç‰¹å¾
        keywords = ['é¡¹ç›®', 'éœ€æ±‚', 'è®¾è®¡', 'å®ç°', 'æµ‹è¯•', 'éƒ¨ç½²', 'é—®é¢˜', 'è§£å†³', 'å­¦ä¹ ', 'å†³ç­–']
        for i, keyword in enumerate(keywords):
            if 38 + i < self.dimension:
                vector[38 + i] = text.count(keyword)
        
        # æ ‡å‡†åŒ–å‘é‡
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _get_candidate_ids(self, category_filter: Optional[str], 
                          tag_filter: Optional[List[str]]) -> Set[str]:
        """è·å–å€™é€‰è®°å¿†ID"""
        if category_filter and tag_filter:
            # åŒæ—¶è¿‡æ»¤åˆ†ç±»å’Œæ ‡ç­¾
            category_ids = self.category_indices.get(category_filter, set())
            tag_ids = set()
            for tag in tag_filter:
                tag_ids.update(self.tag_indices.get(tag, set()))
            return category_ids.intersection(tag_ids)
        
        elif category_filter:
            return self.category_indices.get(category_filter, set())
        
        elif tag_filter:
            tag_ids = set()
            for tag in tag_filter:
                tag_ids.update(self.tag_indices.get(tag, set()))
            return tag_ids
        
        else:
            return set(self.indices.keys())
    
    def _update_importance_sorting(self):
        """æ›´æ–°é‡è¦æ€§æ’åº"""
        self.importance_sorted = sorted(
            self.indices.keys(),
            key=lambda mid: self.indices[mid].importance,
            reverse=True
        )
    
    def _update_average_search_time(self, search_time: float):
        """æ›´æ–°å¹³å‡æœç´¢æ—¶é—´"""
        count = self.stats['search_count']
        current_avg = self.stats['average_search_time']
        self.stats['average_search_time'] = (current_avg * (count - 1) + search_time) / count


class SemanticCache:
    """è¯­ä¹‰ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, max_size: int = 1000, ttl_hours: int = 24):
        self.max_size = max_size
        self.ttl_hours = ttl_hours
        self.cache: OrderedDict[str, SemanticCacheEntry] = OrderedDict()
        self._lock = threading.RLock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'total_queries': 0
        }
    
    def get(self, query: str, similarity_threshold: float = 0.9) -> Optional[List[Dict[str, Any]]]:
        """è·å–ç¼“å­˜ç»“æœ"""
        with self._lock:
            query_hash = self._hash_query(query)
            
            # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
            if query_hash in self.cache:
                entry = self.cache[query_hash]
                if not entry.is_expired(self.ttl_hours):
                    entry.update_access()
                    # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                    self.cache.move_to_end(query_hash)
                    self.stats['hits'] += 1
                    return entry.results
                else:
                    # è¿‡æœŸï¼Œåˆ é™¤
                    del self.cache[query_hash]
            
            # æ£€æŸ¥è¯­ä¹‰ç›¸ä¼¼çš„æŸ¥è¯¢
            for cached_hash, entry in self.cache.items():
                if not entry.is_expired(self.ttl_hours):
                    similarity = self._calculate_query_similarity(query, entry.query_text)
                    if similarity >= similarity_threshold:
                        entry.update_access()
                        self.cache.move_to_end(cached_hash)
                        self.stats['hits'] += 1
                        return entry.results
            
            self.stats['misses'] += 1
            return None
    
    def put(self, query: str, results: List[Dict[str, Any]]):
        """æ·»åŠ ç¼“å­˜ç»“æœ"""
        with self._lock:
            query_hash = self._hash_query(query)
            
            # æ£€æŸ¥å®¹é‡
            if len(self.cache) >= self.max_size:
                # ç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
                oldest_hash, _ = self.cache.popitem(last=False)
                self.stats['evictions'] += 1
            
            # æ·»åŠ æ–°æ¡ç›®
            entry = SemanticCacheEntry(
                query_hash=query_hash,
                query_text=query,
                results=results,
                timestamp=datetime.now()
            )
            entry.update_access()
            
            self.cache[query_hash] = entry
            self.stats['total_queries'] += 1
    
    def clear_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        with self._lock:
            expired_keys = [
                key for key, entry in self.cache.items()
                if entry.is_expired(self.ttl_hours)
            ]
            
            for key in expired_keys:
                del self.cache[key]
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        with self._lock:
            total_requests = self.stats['hits'] + self.stats['misses']
            hit_rate = self.stats['hits'] / max(1, total_requests)
            
            return {
                **self.stats,
                'hit_rate': hit_rate,
                'cache_size': len(self.cache),
                'max_size': self.max_size
            }
    
    def _hash_query(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œ"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()
    
    def _calculate_query_similarity(self, query1: str, query2: str) -> float:
        """è®¡ç®—æŸ¥è¯¢ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€å•çš„åŸºäºè¯æ±‡é‡å çš„ç›¸ä¼¼åº¦è®¡ç®—
        words1 = set(query1.lower().split())
        words2 = set(query2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        # å¢åŠ åŸºäºå­—ç¬¦ç›¸ä¼¼åº¦çš„è®¡ç®—
        char_similarity = self._calculate_char_similarity(query1.lower(), query2.lower())
        word_similarity = len(intersection) / len(union)
        
        # ç»¼åˆç›¸ä¼¼åº¦
        return word_similarity * 0.7 + char_similarity * 0.3
    
    def _calculate_char_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦çº§ç›¸ä¼¼åº¦"""
        if not str1 or not str2:
            return 0.0
        
        # ç®€å•çš„ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        max_len = max(len(str1), len(str2))
        if max_len == 0:
            return 1.0
        
        # è®¡ç®—å…¬å…±å­ä¸²
        common_chars = 0
        for char in set(str1):
            common_chars += min(str1.count(char), str2.count(char))
        
        return common_chars / max_len


class OptimizedMemoryRetrieval:
    """ä¼˜åŒ–çš„è®°å¿†æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, project_id: str = "default", vector_dimension: int = 384):
        self.project_id = project_id
        self.config = get_config()
        
        # æ ¸å¿ƒç»„ä»¶
        self.vector_index = VectorIndexManager(dimension=vector_dimension)
        self.semantic_cache = SemanticCache(max_size=1000, ttl_hours=24)
        
        # è®°å¿†å­˜å‚¨
        self.memories: Dict[str, MemoryFragment] = {}
        self.memory_metadata: Dict[str, Dict[str, Any]] = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_retrievals': 0,
            'cache_hits': 0,
            'vector_searches': 0,
            'average_retrieval_time': 0.0,
            'total_memories': 0
        }
        
        # å­˜å‚¨è·¯å¾„
        self.storage_dir = ensure_directory(Path(self.config.memory_storage_path) / "optimized")
        self.memories_file = self.storage_dir / f"{project_id}_memories.json"
        self.index_file = self.storage_dir / f"{project_id}_vector_index.json"
        
        # åŠ è½½ç°æœ‰æ•°æ®
        self._load_memories_and_index()
        
        print(f"âœ“ ä¼˜åŒ–è®°å¿†æ£€ç´¢ç³»ç»Ÿå·²åˆå§‹åŒ– (é¡¹ç›®: {project_id}, å‘é‡ç»´åº¦: {vector_dimension})")
    
    def add_memory(self, content: str, category: str, importance: float = 0.5, 
                   tags: List[str] = None) -> str:
        """æ·»åŠ è®°å¿†"""
        start_time = time.time()
        
        # ç”Ÿæˆè®°å¿†ID
        memory_id = f"mem_{int(time.time() * 1000)}_{hash(content) % 10000}"
        
        # åˆ›å»ºè®°å¿†ç‰‡æ®µ
        memory = MemoryFragment(
            content=content,
            category=MemoryCategory(category),
            importance=importance,
            tags=tags or [],
            created_at=datetime.now(),
            project_id=self.project_id
        )
        
        # å­˜å‚¨è®°å¿†
        self.memories[memory_id] = memory
        self.memory_metadata[memory_id] = {
            'access_count': 0,
            'last_access': None,
            'creation_time': time.time()
        }
        
        # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
        success = self.vector_index.add_vector(
            memory_id=memory_id,
            content=content,
            category=category,
            importance=importance,
            tags=tags or []
        )
        
        if success:
            self.performance_stats['total_memories'] += 1
            
            # æ¸…ç†è¯­ä¹‰ç¼“å­˜ï¼ˆå› ä¸ºæ·»åŠ äº†æ–°è®°å¿†ï¼‰
            self.semantic_cache.cache.clear()
            
            # ä¿å­˜æ•°æ®
            self._save_memories_and_index()
        
        processing_time = time.time() - start_time
        print(f"âœ“ è®°å¿†å·²æ·»åŠ  (ID: {memory_id}, å¤„ç†æ—¶é—´: {processing_time:.4f}s)")
        
        return memory_id
    
    def search_memories(self, query: str, limit: int = 10, 
                       category: Optional[str] = None,
                       tags: Optional[List[str]] = None,
                       min_similarity: float = 0.3,
                       use_cache: bool = True) -> Dict[str, Any]:
        """æœç´¢è®°å¿†"""
        start_time = time.time()
        self.performance_stats['total_retrievals'] += 1
        
        # å°è¯•ä»è¯­ä¹‰ç¼“å­˜è·å–
        if use_cache:
            cached_results = self.semantic_cache.get(query, similarity_threshold=0.85)
            if cached_results is not None:
                self.performance_stats['cache_hits'] += 1
                processing_time = time.time() - start_time
                self._update_average_retrieval_time(processing_time)
                
                return {
                    'query': query,
                    'results': cached_results,
                    'total_found': len(cached_results),
                    'processing_time': processing_time,
                    'source': 'cache'
                }
        
        # ä½¿ç”¨å‘é‡ç´¢å¼•æœç´¢
        self.performance_stats['vector_searches'] += 1
        
        similar_ids = self.vector_index.search_similar(
            query=query,
            limit=limit * 2,  # è·å–æ›´å¤šå€™é€‰ï¼Œç„¶åè¿‡æ»¤
            category_filter=category,
            tag_filter=tags,
            min_similarity=min_similarity
        )
        
        # æ„å»ºç»“æœ
        results = []
        for memory_id, similarity in similar_ids:
            if memory_id in self.memories:
                memory = self.memories[memory_id]
                metadata = self.memory_metadata.get(memory_id, {})
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                metadata['access_count'] = metadata.get('access_count', 0) + 1
                metadata['last_access'] = datetime.now().isoformat()
                
                result = {
                    'memory_id': memory_id,
                    'content': memory.content,
                    'category': memory.category.value,
                    'importance': memory.importance,
                    'similarity': similarity,
                    'tags': memory.tags,
                    'created_at': memory.created_at.isoformat(),
                    'access_count': metadata['access_count']
                }
                results.append(result)
        
        # æŒ‰ç›¸ä¼¼åº¦å’Œé‡è¦æ€§ç»¼åˆæ’åº
        results.sort(key=lambda x: x['similarity'] * 0.7 + x['importance'] * 0.3, reverse=True)
        results = results[:limit]
        
        # ç¼“å­˜ç»“æœ
        if use_cache and results:
            self.semantic_cache.put(query, results)
        
        processing_time = time.time() - start_time
        self._update_average_retrieval_time(processing_time)
        
        return {
            'query': query,
            'results': results,
            'total_found': len(results),
            'processing_time': processing_time,
            'source': 'vector_search'
        }
    
    def search_memories_optimized(self, query: str, limit: int = 10,
                                 category: Optional[str] = None,
                                 min_similarity: float = 0.3) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–çš„è®°å¿†æœç´¢ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        result = self.search_memories(
            query=query,
            limit=limit,
            category=category,
            min_similarity=min_similarity
        )
        return result['results']
    
    def get_memory_by_id(self, memory_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–è®°å¿†"""
        if memory_id not in self.memories:
            return None
        
        memory = self.memories[memory_id]
        metadata = self.memory_metadata.get(memory_id, {})
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        metadata['access_count'] = metadata.get('access_count', 0) + 1
        metadata['last_access'] = datetime.now().isoformat()
        
        return {
            'memory_id': memory_id,
            'content': memory.content,
            'category': memory.category.value,
            'importance': memory.importance,
            'tags': memory.tags,
            'created_at': memory.created_at.isoformat(),
            'access_count': metadata['access_count']
        }
    
    def get_top_memories(self, limit: int = 10, 
                        category: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–æœ€é‡è¦çš„è®°å¿†"""
        top_ids = self.vector_index.get_top_important(limit=limit, category_filter=category)
        
        results = []
        for memory_id in top_ids:
            if memory_id in self.memories:
                memory_data = self.get_memory_by_id(memory_id)
                if memory_data:
                    results.append(memory_data)
        
        return results
    
    def remove_memory(self, memory_id: str) -> bool:
        """åˆ é™¤è®°å¿†"""
        if memory_id not in self.memories:
            return False
        
        # ä»è®°å¿†å­˜å‚¨ä¸­åˆ é™¤
        del self.memories[memory_id]
        self.memory_metadata.pop(memory_id, None)
        
        # ä»å‘é‡ç´¢å¼•ä¸­åˆ é™¤
        self.vector_index.remove_vector(memory_id)
        
        # æ¸…ç†è¯­ä¹‰ç¼“å­˜
        self.semantic_cache.cache.clear()
        
        # æ›´æ–°ç»Ÿè®¡
        self.performance_stats['total_memories'] -= 1
        
        # ä¿å­˜æ•°æ®
        self._save_memories_and_index()
        
        return True
    
    def optimize_indices(self):
        """ä¼˜åŒ–ç´¢å¼•"""
        print("ğŸ”§ å¼€å§‹ç´¢å¼•ä¼˜åŒ–...")
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self.semantic_cache.clear_expired()
        
        # é‡å»ºå‘é‡ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if len(self.memories) != self.vector_index.stats['total_vectors']:
            print("  - é‡å»ºå‘é‡ç´¢å¼•...")
            self._rebuild_vector_index()
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        self._optimize_memory_usage()
        
        print("âœ“ ç´¢å¼•ä¼˜åŒ–å®Œæˆ")
    
    def benchmark_performance(self, num_queries: int = 100) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"ğŸƒ å¼€å§‹è®°å¿†æ£€ç´¢æ€§èƒ½åŸºå‡†æµ‹è¯• ({num_queries} æ¬¡æŸ¥è¯¢)...")
        
        # å‡†å¤‡æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "é¡¹ç›®éœ€æ±‚åˆ†æ",
            "ç³»ç»Ÿæ¶æ„è®¾è®¡",
            "æ•°æ®åº“ä¼˜åŒ–",
            "ç”¨æˆ·ç•Œé¢è®¾è®¡",
            "æ€§èƒ½æµ‹è¯•",
            "éƒ¨ç½²é…ç½®",
            "é”™è¯¯å¤„ç†",
            "å®‰å…¨è€ƒè™‘",
            "ä»£ç é‡æ„",
            "æ–‡æ¡£ç¼–å†™"
        ]
        
        # æ‰©å±•æŸ¥è¯¢åˆ—è¡¨
        extended_queries = []
        for i in range(num_queries):
            base_query = test_queries[i % len(test_queries)]
            extended_queries.append(f"{base_query} {i}")
        
        # æµ‹è¯•å‘é‡æœç´¢æ€§èƒ½
        vector_times = []
        for query in extended_queries:
            start_time = time.time()
            self.search_memories(query, limit=5, use_cache=False)
            vector_times.append(time.time() - start_time)
        
        # æµ‹è¯•ç¼“å­˜æ€§èƒ½
        cache_times = []
        for query in extended_queries[:num_queries//2]:  # é‡å¤æŸ¥è¯¢æµ‹è¯•ç¼“å­˜
            start_time = time.time()
            self.search_memories(query, limit=5, use_cache=True)
            cache_times.append(time.time() - start_time)
        
        # è®¡ç®—ç»Ÿè®¡
        avg_vector_time = sum(vector_times) / len(vector_times)
        avg_cache_time = sum(cache_times) / len(cache_times)
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.semantic_cache.get_stats()
        vector_stats = self.vector_index.get_stats()
        
        benchmark_result = {
            'average_search_time': avg_vector_time,
            'average_cache_time': avg_cache_time,
            'queries_per_second': 1.0 / avg_vector_time,
            'cache_hit_rate': cache_stats['hit_rate'],
            'cache_speedup': avg_vector_time / max(0.001, avg_cache_time),
            'total_memories': self.performance_stats['total_memories'],
            'vector_dimension': self.vector_index.dimension,
            'performance_grade': self._calculate_performance_grade(avg_vector_time)
        }
        
        print(f"âœ“ åŸºå‡†æµ‹è¯•å®Œæˆ:")
        print(f"  - å¹³å‡æœç´¢æ—¶é—´: {avg_vector_time:.6f}s")
        print(f"  - å¹³å‡ç¼“å­˜æ—¶é—´: {avg_cache_time:.6f}s")
        print(f"  - æ¯ç§’æŸ¥è¯¢æ•°: {benchmark_result['queries_per_second']:.1f}")
        print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
        print(f"  - ç¼“å­˜åŠ é€Ÿæ¯”: {benchmark_result['cache_speedup']:.1f}x")
        print(f"  - æ€§èƒ½ç­‰çº§: {benchmark_result['performance_grade']}")
        
        return benchmark_result
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return {
            'retrieval_stats': self.performance_stats,
            'vector_index_stats': self.vector_index.get_stats(),
            'semantic_cache_stats': self.semantic_cache.get_stats(),
            'memory_count': len(self.memories),
            'index_health': self._assess_index_health()
        }
    
    def _rebuild_vector_index(self):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        self.vector_index = VectorIndexManager(dimension=self.vector_index.dimension)
        
        # é‡æ–°æ·»åŠ æ‰€æœ‰è®°å¿†
        for memory_id, memory in self.memories.items():
            self.vector_index.add_vector(
                memory_id=memory_id,
                content=memory.content,
                category=memory.category.value,
                importance=memory.importance,
                tags=memory.tags
            )
    
    def _optimize_memory_usage(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # æ¸…ç†é•¿æ—¶é—´æœªè®¿é—®çš„è®°å¿†å…ƒæ•°æ®
        current_time = time.time()
        cutoff_time = current_time - (30 * 24 * 3600)  # 30å¤©
        
        to_remove = []
        for memory_id, metadata in self.memory_metadata.items():
            last_access = metadata.get('last_access')
            if last_access:
                try:
                    last_access_time = datetime.fromisoformat(last_access).timestamp()
                    if last_access_time < cutoff_time:
                        to_remove.append(memory_id)
                except:
                    pass
        
        # ç§»é™¤è¿‡æœŸå…ƒæ•°æ®ï¼ˆä½†ä¿ç•™è®°å¿†æœ¬èº«ï¼‰
        for memory_id in to_remove:
            if self.memory_metadata[memory_id].get('access_count', 0) == 0:
                # åªç§»é™¤ä»æœªè¢«è®¿é—®çš„è®°å¿†çš„å…ƒæ•°æ®
                self.memory_metadata.pop(memory_id, None)
    
    def _update_average_retrieval_time(self, retrieval_time: float):
        """æ›´æ–°å¹³å‡æ£€ç´¢æ—¶é—´"""
        count = self.performance_stats['total_retrievals']
        current_avg = self.performance_stats['average_retrieval_time']
        
        self.performance_stats['average_retrieval_time'] = (
            (current_avg * (count - 1) + retrieval_time) / count
        )
    
    def _calculate_performance_grade(self, avg_time: float) -> str:
        """è®¡ç®—æ€§èƒ½ç­‰çº§"""
        if avg_time < 0.001:
            return 'A+'
        elif avg_time < 0.005:
            return 'A'
        elif avg_time < 0.01:
            return 'B'
        elif avg_time < 0.05:
            return 'C'
        else:
            return 'D'
    
    def _assess_index_health(self) -> Dict[str, Any]:
        """è¯„ä¼°ç´¢å¼•å¥åº·åº¦"""
        vector_stats = self.vector_index.get_stats()
        cache_stats = self.semantic_cache.get_stats()
        
        # è®¡ç®—å¥åº·åˆ†æ•°
        vector_health = min(1.0, vector_stats['total_vectors'] / max(1, len(self.memories)))
        cache_health = cache_stats['hit_rate']
        search_health = min(1.0, 1.0 / max(0.001, vector_stats['average_search_time']))
        
        overall_health = (vector_health * 0.4 + cache_health * 0.3 + search_health * 0.3)
        
        return {
            'overall_health': overall_health,
            'vector_index_health': vector_health,
            'cache_health': cache_health,
            'search_performance_health': search_health,
            'status': 'excellent' if overall_health > 0.8 else 'good' if overall_health > 0.6 else 'fair'
        }
    
    def _load_memories_and_index(self):
        """åŠ è½½è®°å¿†å’Œç´¢å¼•"""
        try:
            # åŠ è½½è®°å¿†
            if self.memories_file.exists():
                with open(self.memories_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # æ¢å¤è®°å¿†
                    for memory_id, memory_data in data.get('memories', {}).items():
                        memory = MemoryFragment(
                            content=memory_data['content'],
                            category=MemoryCategory(memory_data['category']),
                            importance=memory_data['importance'],
                            tags=memory_data.get('tags', []),
                            created_at=datetime.fromisoformat(memory_data['created_at']),
                            project_id=memory_data.get('project_id', self.project_id)
                        )
                        self.memories[memory_id] = memory
                    
                    # æ¢å¤å…ƒæ•°æ®
                    self.memory_metadata = data.get('metadata', {})
                    self.performance_stats.update(data.get('performance_stats', {}))
            
            # åŠ è½½å‘é‡ç´¢å¼•
            if self.index_file.exists():
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                    
                    # æ¢å¤å‘é‡ç´¢å¼•
                    for memory_id, vector_data in index_data.get('indices', {}).items():
                        try:
                            vector_index = VectorIndex.from_dict(vector_data)
                            self.vector_index.indices[memory_id] = vector_index
                            
                            # é‡å»ºåˆ†ç±»å’Œæ ‡ç­¾ç´¢å¼•
                            self.vector_index.category_indices[vector_index.category].add(memory_id)
                            for tag in vector_index.tags:
                                self.vector_index.tag_indices[tag].add(memory_id)
                        except Exception as e:
                            print(f"æ¢å¤å‘é‡ç´¢å¼•å¤±è´¥ {memory_id}: {e}")
                    
                    # æ›´æ–°ç»Ÿè®¡å’Œæ’åº
                    self.vector_index.stats.update(index_data.get('stats', {}))
                    self.vector_index._update_importance_sorting()
            
            # å¦‚æœè®°å¿†å’Œç´¢å¼•ä¸åŒ¹é…ï¼Œé‡å»ºç´¢å¼•
            if len(self.memories) != len(self.vector_index.indices):
                print("âš ï¸ è®°å¿†å’Œç´¢å¼•ä¸åŒ¹é…ï¼Œé‡å»ºå‘é‡ç´¢å¼•...")
                self._rebuild_vector_index()
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è®°å¿†å’Œç´¢å¼•å¤±è´¥: {e}")
    
    def _save_memories_and_index(self):
        """ä¿å­˜è®°å¿†å’Œç´¢å¼•"""
        try:
            # ä¿å­˜è®°å¿†
            memories_data = {
                'memories': {
                    memory_id: {
                        'content': memory.content,
                        'category': memory.category.value,
                        'importance': memory.importance,
                        'tags': memory.tags,
                        'created_at': memory.created_at.isoformat(),
                        'project_id': memory.project_id
                    }
                    for memory_id, memory in self.memories.items()
                },
                'metadata': self.memory_metadata,
                'performance_stats': self.performance_stats,
                'last_saved': datetime.now().isoformat()
            }
            
            with open(self.memories_file, 'w', encoding='utf-8') as f:
                json.dump(memories_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å‘é‡ç´¢å¼•
            index_data = {
                'indices': {
                    memory_id: vector_index.to_dict()
                    for memory_id, vector_index in self.vector_index.indices.items()
                },
                'stats': self.vector_index.stats,
                'last_saved': datetime.now().isoformat()
            }
            
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®°å¿†å’Œç´¢å¼•å¤±è´¥: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿æ•°æ®ä¿å­˜"""
        try:
            self._save_memories_and_index()
        except:
            pass